{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Authors</b>: [Alexandra Bradan](https://github.com/alexandrabradan), [Alice Graziani](https://github.com/alicegraziani25) and [Eleonora Cocciu](https://github.com/eleonoracocciu)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 21/05/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pickle\n",
    "import isoweek\n",
    "import datetime\n",
    "import calendar\n",
    "from datetime import date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "# useful libraries\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import statistics\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import percentile\n",
    "\n",
    "# visualisarion\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import Image\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# dimensional reducers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif  # classification\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression  # regression\n",
    "\n",
    "# scalers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# performance visualisation \n",
    "from sklearn import tree\n",
    "from scikitplot.metrics import plot_roc\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "%matplotlib inline\n",
    "\n",
    "from yellowbrick.style import set_palette\n",
    "set_palette('bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_column_name = 'frequency'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Datasets loading </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(103708, 44)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../../data/fma_metadata/group_20_fma.csv', index_col=0)\n",
    "tracks = pd.read_csv('../../data/fma_metadata/tracks.csv', index_col=0, header=[0, 1])\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Recompose track_date_created column</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_created_column = pd.to_datetime(tracks[('track', 'date_created')], format='%Y%m', errors='ignore').astype('datetime64[ns]')\n",
    "track_date_created_year_index = df.columns.get_loc(\"track_date_created_year\")\n",
    "track_date_created_season_index = df.columns.get_loc(\"track_date_created_season\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(track_date_created_year_index, 'track_date_created', date_created_column)\n",
    "del df[\"track_date_created_year\"]\n",
    "del df[\"track_date_created_season\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Marginal datetime records removal </h6>\n",
    "\n",
    "Track creation spans between **2008-11-25 17:49:06** and **2017-03-30 15:23:39**, but marginal dates of such periods were deleted, since it was noticed that they were incomplete. So we analyzed the period spanning between **2008-12-29 00:00:00 and 2017-01-01 00:00:00**, reshaped by making each day having 24 hours and adding 186 missing days. The last three days of 2008 were approximated to 2009 since the were respectivelly Monday (29/12), Tuesday (30/12) and Wendsday (31/12), while the fisrt day of 2017 was approximated with 2016 since it was a Sunday (01/01).\n",
    "\n",
    "By removing marginal dates to have 8 complete years, we got rid of and 5.34% of Studio Recording tracks (2008: {11: 1263, 12: 28} and 2017: {1: 995, 2: 1619, 3: 1023}) and 16.32% of Live Recording tracks (2008:Counter({11: 186, 12: 1139}) and 2017: {1: 223, 2: 212, 3: 121}). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# approximated marginal weeks\n",
    "first_year_keep = df[(df['track_date_created'].dt.year == 2008) & \n",
    "                    (df['track_date_created'].dt.month == 12) & \n",
    "                    (df['track_date_created'].dt.day >= 29)] \n",
    "\n",
    "\n",
    "last_year_keep = df[(df['track_date_created'].dt.year == 2017) & \n",
    "                    (df['track_date_created'].dt.month == 1) & \n",
    "                    (df['track_date_created'].dt.day == 1)]\n",
    "\n",
    "to_keep = set(list(first_year_keep.index) + list(last_year_keep.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_year = df[df['track_date_created'].dt.year == 2008] \n",
    "last_year = df[df['track_date_created'].dt.year == 2017]\n",
    "first_year_studio = first_year[first_year['album_type'] == \"Studio Recording\"]\n",
    "last_year_studio = last_year[last_year['album_type'] == \"Studio Recording\"]\n",
    "first_year_live = first_year[first_year['album_type'] == \"Live Recording\"]\n",
    "last_year_live = last_year[last_year['album_type'] == \"Live Recording\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Studio removed records 4917\n",
      "Live removed records 1881\n",
      "total removed records 6798\n"
     ]
    }
   ],
   "source": [
    "print(\"Studio removed records\", len((set(first_year_studio.index).union(set(last_year_studio.index))).difference(to_keep)))\n",
    "print(\"Live removed records\", len((set(first_year_live.index).union(set(last_year_live.index))).difference(to_keep)))\n",
    "print(\"total removed records\", len((set(first_year.index).union(set(last_year.index))).difference(to_keep)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96910, 43)"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing marginal weeks, months and years\n",
    "to_drop = set(list(first_year.index) + list(last_year.index)).difference(to_keep)\n",
    "df.drop(to_drop, axis=0, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Create TS DataFrame </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "# truncated datetime at hours (not interested in minutes, seconds and microsenconds)\n",
    "track_date_created = []\n",
    "for d in df['track_date_created']:\n",
    "    d_approximated = d.replace(minute=0, second=0, microsecond=0)\n",
    "    track_date_created.append(d_approximated)\n",
    "    \n",
    "# create a new temporal df\n",
    "data = {'track_date_created': track_date_created, 'album_type': df['album_type'].values}\n",
    "tmp_df_ts = pd.DataFrame(data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2008-12-30 07:00:00 2017-01-01 21:00:00\n",
      "days between start and end: 2925\n"
     ]
    }
   ],
   "source": [
    "# check id exist missing days\n",
    "start = tmp_df_ts['track_date_created'].min()\n",
    "end = tmp_df_ts['track_date_created'].max()\n",
    "print(start, end)\n",
    "\n",
    "delta = end - start\n",
    "print('days between start and end:', delta.days + 1)  \n",
    "\n",
    "# encode yearly days as contigous integers \n",
    "days = {}\n",
    "for i in range(0, delta.days + 1):  # adding 1 extra days to contemplate end\n",
    "    day = start + timedelta(days=i)\n",
    "    key = day.year, day.month, day.day\n",
    "    days[key] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing_days 187\n"
     ]
    }
   ],
   "source": [
    "# create final df, without sequential hours\n",
    "track_date_created_dict = {}\n",
    "for t in tmp_df_ts['track_date_created'].unique():\n",
    "    counter = Counter(tmp_df_ts[tmp_df_ts['track_date_created'] == t]['album_type'].values)\n",
    "    t = pd.Timestamp(t)\n",
    "    try:\n",
    "        tmp = track_date_created_dict[t]\n",
    "    except KeyError:\n",
    "        track_date_created_dict[t] = {}\n",
    "        track_date_created_dict[t]['Live Recording'] = 0\n",
    "        track_date_created_dict[t]['Studio Recording'] = 0\n",
    "        \n",
    "    for key, value in counter.items():\n",
    "        track_date_created_dict[t][key] += value\n",
    "        \n",
    "# add missing days to DataFrame\n",
    "current_days = set()\n",
    "for t in track_date_created_dict.keys():\n",
    "    key = t.year, t.month, t.day\n",
    "    current_days.add(key)\n",
    "        \n",
    "missing_days = set(days).difference(current_days)\n",
    "missing_days.add((2008, 12, 29))\n",
    "print(\"missing_days\", len(missing_days))\n",
    "\n",
    "for t in missing_days:\n",
    "    t = pd.Timestamp(t[0], t[1], t[2], 0)\n",
    "    track_date_created_dict[t] = {}\n",
    "    track_date_created_dict[t]['Live Recording'] = 0\n",
    "    track_date_created_dict[t]['Studio Recording'] = 0\n",
    "\n",
    "data = {'track_date_created': list(track_date_created_dict.keys()), \n",
    "        'Studio_Recording_' + ts_column_name: [x['Studio Recording'] for x in list(track_date_created_dict.values())], \n",
    "        'Live_Recording_' + ts_column_name: [x['Live Recording'] for x in list(track_date_created_dict.values())]}\n",
    "\n",
    "df_ts = pd.DataFrame(data=data)\n",
    "df_ts = df_ts.set_index('track_date_created')\n",
    "df_ts = df_ts.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70224, 2)"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to make all days have the same amouth of hours [0-23]\n",
    "keys = list(track_date_created_dict.keys())\n",
    "keys_hours = {}\n",
    "for t in keys:\n",
    "    try:\n",
    "        tmp_list = keys_hours[t.year, t.month, t.day]\n",
    "        tmp_list.append(t.hour)\n",
    "        keys_hours[t.year, t.month, t.day] = tmp_list\n",
    "    except KeyError:\n",
    "        keys_hours[t.year, t.month, t.day] = [t.hour]\n",
    "\n",
    "\n",
    "# create final df, with sequential hours\n",
    "new_track_date_created_dict = {}\n",
    "for key, value in keys_hours.items():\n",
    "    hours = keys_hours[key[0], key[1], key[2]]\n",
    "\n",
    "    k = key[0], key[1], key[2]\n",
    "    for h in range(0, 24):\n",
    "        t = pd.Timestamp(k[0], k[1], k[2], h)\n",
    "        \n",
    "        if h in hours:\n",
    "            counter = {'Live Recording': track_date_created_dict[t]['Live Recording'],\n",
    "                       'Studio Recording': track_date_created_dict[t]['Studio Recording']}\n",
    "        else:\n",
    "            counter = {'Live Recording': 0,\n",
    "                       'Studio Recording': 0}\n",
    "        try:\n",
    "            tmp = new_track_date_created_dict[t]\n",
    "        except KeyError:\n",
    "            new_track_date_created_dict[t] = {}\n",
    "            new_track_date_created_dict[t]['Live Recording'] = 0\n",
    "            new_track_date_created_dict[t]['Studio Recording'] = 0\n",
    "\n",
    "        for key, value in counter.items():\n",
    "            new_track_date_created_dict[t][key] += value\n",
    "            \n",
    "data = {'track_date_created': list(new_track_date_created_dict.keys()), \n",
    "        'Studio_Recording_' + ts_column_name: [x['Studio Recording'] for x in list(new_track_date_created_dict.values())], \n",
    "        'Live_Recording_' + ts_column_name: [x['Live Recording'] for x in list(new_track_date_created_dict.values())]}\n",
    "final_df_ts = pd.DataFrame(data=data)\n",
    "final_df_ts = final_df_ts.set_index('track_date_created')\n",
    "final_df_ts = final_df_ts.sort_index()\n",
    "final_df_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Saving DataFrame on file </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"TS_album_type_\" + ts_column_name + \".csv\"\n",
    "final_df_ts.to_csv('../../data/fma_metadata/' + filename, index=True, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
