{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Authors</b>: [Alexandra Bradan](https://github.com/alexandrabradan), [Alice Graziani](https://github.com/alicegraziani25) and [Eleonora Cocciu](https://github.com/eleonoracocciu)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 21/05/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# useful libraries\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import statistics\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import percentile\n",
    "\n",
    "# visualisarion\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import Image\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# dimensional reducers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif  # classification\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression  # regression\n",
    "\n",
    "# scalers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# performance visualisation \n",
    "from sklearn import tree\n",
    "from scikitplot.metrics import plot_roc\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# tree classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# linear classifiers\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# neighbors classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# naive_bayes classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# ensemble classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "%matplotlib inline\n",
    "\n",
    "from yellowbrick.style import set_palette\n",
    "set_palette('bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Global parameters </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "scoring = 'f1_weighted'\n",
    "random_state = 42\n",
    "\n",
    "# test_n_splits = 9\n",
    "test_n_splits = 3\n",
    "\n",
    "model = BaggingClassifier(random_state=random_state, n_jobs=-1) #  base_estimator=DecisionTreeClassifier  \n",
    "model_name = \"DecisionTreeClassifier BaggingClassifier\"\n",
    "\n",
    "learning_curve_flag = False\n",
    "v_or_t_flag = \"TST\"\n",
    "cmap = plt.cm.Greens\n",
    "color = \"green\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Datasets loading </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 55) (10874, 55)\n",
      "(92834, 1) (10874, 1)\n",
      "(92834, 28) (10874, 28)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('../../data/fma_metadata/X_train_merged.csv', index_col=0)\n",
    "X_test = pd.read_csv('../../data/fma_metadata/X_test.csv', index_col=0)\n",
    "\n",
    "y_train = pd.read_csv('../../data/fma_metadata/y_train_merged.csv', index_col=0)\n",
    "y_test = pd.read_csv('../../data/fma_metadata/y_test.csv', index_col=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train_vt = pd.read_csv('../../data/fma_metadata/X_train_merged_vt.csv', index_col=0)\n",
    "X_test_vt = pd.read_csv('../../data/fma_metadata/X_test_vt.csv', index_col=0)\n",
    "\n",
    "print(X_train_vt.shape, X_test_vt.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Continous, categorical/ordinal column retrieval</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns 37\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = []  # continous variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" not in column_name) and  \\\n",
    "          (\"track_date_created_year\" not in column_name) and \\\n",
    "            (\"track_date_created_season\" not in column_name):\n",
    "                numeric_columns.append(column_name)\n",
    "print(\"numeric_columns\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns 18\n"
     ]
    }
   ],
   "source": [
    "categoric_columns = []  # ordinal or categorical variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" in column_name) or  \\\n",
    "          (\"track_date_created_year\" in column_name) or \\\n",
    "            (\"track_date_created_season\" in column_name):\n",
    "                categoric_columns.append(column_name)\n",
    "print(\"categoric_columns\", len(categoric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns_vt 15\n"
     ]
    }
   ],
   "source": [
    "numeric_columns_vt = list(set(numeric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"numeric_columns_vt\", len(numeric_columns_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns_vt 13\n"
     ]
    }
   ],
   "source": [
    "categoric_columns_vt = list(set(categoric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"categoric_columns_vt\", len(categoric_columns_vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Define current (filtered) train and test</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 37) (10874, 37) (92834, 15) (10874, 15)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_train[numeric_columns].copy()\n",
    "y_tr = y_train.copy()\n",
    "X_ts = X_test[numeric_columns].copy()\n",
    "y_ts = y_test.copy()\n",
    "\n",
    "X_tr_vt = X_train_vt[numeric_columns_vt].copy()\n",
    "X_ts_vt = X_test_vt[numeric_columns_vt].copy()\n",
    "\n",
    "print(X_tr.shape, X_ts.shape, X_tr_vt.shape, X_ts_vt.shape)\n",
    "print(y_tr.shape, y_ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>BaggingClassifier (baseline classifier)</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_baseline_classification(X_tr, y_tr, X_ts, y_ts):\n",
    "    fitted_model = model.fit(X_tr, y_tr.values.ravel())\n",
    "    y_pred = model.predict(X_ts)\n",
    "    \n",
    "    print(confusion_matrix(y_ts, y_pred))\n",
    "    print(classification_report(y_ts, y_pred))\n",
    "    \n",
    "    try:\n",
    "        features_importance = sorted(zip(X_tr.columns, fitted_model.feature_importances_),reverse=True)\n",
    "        # print(features_importance)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        features_importance = sorted(zip(X_tr.columns, fitted_model.coef_),reverse=True)\n",
    "        # print(features_importance)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>All features (categorica, ordinal and continous)</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9589  205]\n",
      " [ 649  431]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.98      0.96      9794\n",
      "           1       0.68      0.40      0.50      1080\n",
      "\n",
      "    accuracy                           0.92     10874\n",
      "   macro avg       0.81      0.69      0.73     10874\n",
      "weighted avg       0.91      0.92      0.91     10874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_baseline_classification(X_train, \n",
    "                              y_train, \n",
    "                              X_test, \n",
    "                              y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Only continous variables</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9560  234]\n",
      " [ 747  333]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.98      0.95      9794\n",
      "           1       0.59      0.31      0.40      1080\n",
      "\n",
      "    accuracy                           0.91     10874\n",
      "   macro avg       0.76      0.64      0.68     10874\n",
      "weighted avg       0.89      0.91      0.90     10874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_baseline_classification(X_train[numeric_columns], \n",
    "                              y_train, \n",
    "                              X_test[numeric_columns], \n",
    "                              y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Only categorical or ordinal variables</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9642  152]\n",
      " [ 958  122]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.98      0.95      9794\n",
      "           1       0.45      0.11      0.18      1080\n",
      "\n",
      "    accuracy                           0.90     10874\n",
      "   macro avg       0.68      0.55      0.56     10874\n",
      "weighted avg       0.86      0.90      0.87     10874\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_baseline_classification(X_train[categoric_columns], \n",
    "                              y_train, \n",
    "                              X_test[categoric_columns], \n",
    "                              y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Tuned BaggingClassifier </h2>\n",
    "A Bagging classifier is an ensemble meta-estimator that fits base classifiers each on random subsets of the original dataset and then aggregate their individual predictions (either by voting or by averaging) to form a final prediction. Such a meta-estimator can typically be used as a way to reduce the variance of a black-box estimator (e.g., a decision tree), by introducing randomization into its construction procedure and then making an ensemble out of it.\n",
    "\n",
    "This algorithm encompasses several works from the literature:\n",
    "- **Pasting**: when random subsets of the dataset are drawn as random subsets of the samples;\n",
    "- **Bagging**: if samples are drawn with replacement;\n",
    "- **Random Subspaces**: when random subsets of the dataset are drawn as random subsets of the features;\n",
    "- **Random Patches**: when base estimators are built on subsets of both samples and features.\n",
    "\n",
    "\n",
    "\n",
    "**Default parameters**:\n",
    "- base_estimator=None, \n",
    "- n_estimators=10, *, \n",
    "- max_samples=1.0, \n",
    "- max_features=1.0, \n",
    "- bootstrap=True, \n",
    "- bootstrap_features=False, \n",
    "- oob_score=False, \n",
    "- warm_start=False, \n",
    "- n_jobs=None, \n",
    "- random_state=None, \n",
    "- verbose=0\n",
    "\n",
    "**Tuned parameters**:\n",
    "- random_state=42\n",
    "- n_jobs=None\n",
    "- n_estimators=stump classifier\n",
    "- n_estimators=range(50, 501, 50) + [25, 1000], \n",
    "- max_samples=[sqrt_records] + list(np.arange(0.01, 0.1, 0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_grid(X_tr):\n",
    "    grid = dict()\n",
    "    sqrt_records = round(math.sqrt(X_tr.shape[0]))\n",
    "    grid['model__n_estimators'] = list(range(50, 501, 50)) + [25, 1000]\n",
    "    grid['model__max_samples'] = [sqrt_records] + list(np.arange(0.01, 0.1, 0.01))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FUNCTIONS\n",
    "def plot_roc_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_roc(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s ROC curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_precision_recall_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_precision_recall(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Precision-Recall curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cumulative_gain_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_cumulative_gain(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Cumulative Gains curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lift_curve_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_lift_curve(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Lift curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"%s\\'s %s classification report\" % (model_name, v_or_t_flag.upper()))\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_classification_report(y_ts, y_pred):\n",
    "    model_report = classification_report(y_ts, \n",
    "                                       y_pred,\n",
    "                                       # target_names=['Studio Recording', 'Live Recording'],\n",
    "                                       output_dict=True)\n",
    "    model_r = pd.DataFrame(model_report).iloc[:-1, :].T\n",
    "    sns.heatmap(model_r, annot=True, cmap=cmap, cbar=False)\n",
    "    plt.title(\"%s\\'s %s confusion matrix\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_decision_boundary(X_tr, y_tr, scaler, model):\n",
    "    try:\n",
    "        pca = PCA(n_components=2)\n",
    "        best_visualisation_scaler = StandardScaler()\n",
    "        scaled_X_tr = best_visualisation_scaler.fit_transform(X_tr)\n",
    "        X = pca.fit_transform(scaled_X_tr)\n",
    "        # X = pca.fit_transform(X_tr)\n",
    "        y = y_tr.values.ravel()\n",
    "\n",
    "        model.fit(X, y)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        fig = plot_decision_regions(X=X, y=y, clf=model, legend=2)\n",
    "        plt.xlabel(\"PCA component 1\")\n",
    "        plt.ylabel(\"PCA component 2\")\n",
    "        plt.title(\"%s's %s decision boundary\" % (model_name, v_or_t_flag))\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "def spot_errors(test_label, test_pred):  \n",
    "    spot_errors = []\n",
    "    label_errors = []\n",
    "    for i in range(len(test_label)):\n",
    "        if test_label[i] != test_pred[i]:\n",
    "            spot_errors.append('darkred')\n",
    "            label_errors.append(\"wrong prediction\")\n",
    "        else:\n",
    "            spot_errors.append('darkgray')\n",
    "            label_errors.append(\"correct prediction\")\n",
    "    return spot_errors, label_errors\n",
    "\n",
    "def classification_visualizer(test_set, test_label, test_pred):\n",
    "    test_label = test_label.values\n",
    "    \n",
    "    f, axs = plt.subplots(nrows=1, ncols=3, figsize=(24,8))\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    labels = [test_label, test_pred, errors]\n",
    "    titles = ['True Labels', 'Predicted Labels', 'Misclassifications']\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        axs[i].scatter(test_set[missclassif_column_name1], test_set[missclassif_column_name2], c=labels[i], cmap='cividis')\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlabel(missclassif_column_name1, fontdict={'fontsize': 'large'})\n",
    "        axs[i].set_ylabel(missclassif_column_name2, fontdict={'fontsize': 'large'})\n",
    "        \n",
    "    plt.suptitle('Visualization of the ' + model_name + ' classifier on the %s' % v_or_t_flag)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def error_visualizer(test_set, test_label, test_pred, column_name1, column_name2):\n",
    "    test_label = test_label.values\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    \n",
    "    palette = ['darkgray', 'darkred']\n",
    "    if errors[0] == 'darkred':\n",
    "        palette = ['darkred', 'darkgray']\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 6))\n",
    "    sns.scatterplot(x=test_set[column_name1], y=test_set[column_name2], hue=label_errors, palette=palette)\n",
    "    plt.title('%s\\'s %s misclassifications' % (model_name, v_or_t_flag))\n",
    "    plt.xlabel(column_name1)\n",
    "    plt.ylabel(column_name2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_learning_curve(X_tr, y_tr, model, v_or_t_flag):\n",
    "    second_score = \"\"\n",
    "    if v_or_t_flag == 'VAL':\n",
    "        cv = StratifiedKFold(n_splits=validation_n_splits)\n",
    "        second_score = \"Validation score\"\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "        second_score = \"Test score\"\n",
    "    sizes = np.linspace(0.3, 1.0, 10)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    visualizer = LearningCurve(model, cv=cv, scoring=scoring, train_sizes=sizes, \n",
    "                                                                       n_jobs=-1, random_state=random_state)\n",
    "\n",
    "    visualizer.fit(X_tr, y_tr.values.ravel())    \n",
    "    # visualizer.ax.get_lines()[1].set_label(second_score)\n",
    "    visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features_grid_cv(X_tr, y_tr, results, key):\n",
    "    \n",
    "    if key == 'anova':\n",
    "        best_k = results.best_params_['anova__k']\n",
    "        select_k_best = SelectKBest(score_func=f_classif, k=best_k)\n",
    "        fit = select_k_best.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.scores_)\n",
    "        df_columns = pd.DataFrame(X_tr.columns)\n",
    "    elif key == 'rfe':\n",
    "        best_k = results.best_params_['rfe__n_features_to_select']\n",
    "        estimator = results.best_params_['rfe__estimator']\n",
    "        select_rfe = RFE(estimator=estimator, n_features_to_select=best_k)  # best_k=estimator\n",
    "        fit = select_rfe.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.ranking_)\n",
    "    else:\n",
    "        print(\"wrong key=%s\" % key)\n",
    "        sys.exit(-1)\n",
    "  \n",
    "    df_columns = pd.DataFrame(X_tr.columns)\n",
    "    feature_scores = pd.concat([df_columns, df_scores],axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance using %s\" % (model_name, v_or_t_flag,  key.upper()))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances_or_coef(X_tr, y_tr, tuned_model):\n",
    "\n",
    "    try:\n",
    "        df_scores = pd.DataFrame(tuned_model.feature_importances_)\n",
    "        # best_k = tuned_model.n_features_\n",
    "        best_k =X_tr.shape[0]\n",
    "    except:\n",
    "        try:\n",
    "            df_scores = pd.DataFrame(tuned_model.coef_)\n",
    "\n",
    "            best_k = len([x for x in list(df_scores.values)])\n",
    "        except:\n",
    "            print(\"Wrong curr_model's retrieval feature importance\")\n",
    "            sys.exit(-1)\n",
    "\n",
    "    df_columns = pd.DataFrame(X_tr.columns)   \n",
    "    feature_scores = pd.concat([df_columns, df_scores], axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance\" % (model_name, v_or_t_flag))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_model(X_tr, y_tr, params):\n",
    "    \n",
    "    X_tr_curr = X_tr.copy()\n",
    "    \n",
    "    model_params = list(model_grid(X_tr_curr).keys())\n",
    "    try:\n",
    "        n_bins = params['preprocessor__numeric__discretizer__n_bins'] \n",
    "        strategy = params['preprocessor__numeric__discretizer__strategy']\n",
    "        encode = params['preprocessor__numeric__discretizer__encode']\n",
    "        discretizer = KBinsDiscretizer(encode=encode, n_bins=n_bins, strategy=strategy)\n",
    "        # scale data\n",
    "        X_tr_curr = discretizer.fit_transform(X_tr_curr.values)\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        scaler = params['preprocessor__numeric__discretizer__scaler']\n",
    "        # scale data\n",
    "        X_tr_curr = scaler.fit_transform(X_tr_curr.values)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # retrieve best hyperameters\n",
    "    tmp_model_hyperparameters = dict((k, params[k]) for k in model_params if k in params)\n",
    "    model_hyperparameters = {}\n",
    "    for key, value in tmp_model_hyperparameters.items():\n",
    "        key = key.split('model__')[1].replace(\"'\", \"\")\n",
    "        model_hyperparameters[key] = value\n",
    "        \n",
    "    tuned_model =  model.set_params(**model_hyperparameters)\n",
    "    tuned_model.fit(X_tr_curr, y_tr.values.ravel())\n",
    "    \n",
    "    plot_decision_boundary(X_tr_curr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    \n",
    "    if learning_curve_flag:\n",
    "        plot_learning_curve(curr_X_tr_vt, y_tr, tuned_model, v_or_t_flag)\n",
    "    \n",
    "    return tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_tr, y_tr, X_ts, y_ts, numeric_features, categorical_features, discretizer_flag, \n",
    "                                                            scaler_flag, feature_filter_key, feature_flag):\n",
    "    \n",
    "    # define the evaluation method\n",
    "    cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "\n",
    "    # construct the pipeline to evaluate\n",
    "    # scaler = RobustScaler()\n",
    "    grid=model_grid(X_tr)\n",
    "    steps = [('model', model)]\n",
    "        \n",
    "    if feature_filter_key == 'anova':\n",
    "        anova = SelectKBest(score_func=f_classif)\n",
    "        steps.insert(0, ('anova', anova))\n",
    "        grid['anova__k'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    elif feature_filter_key == 'rfe':\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier())\n",
    "        steps.insert(0, ('rfe', rfe))\n",
    "        grid['rfe__estimator'] = [DecisionTreeClassifier(), LogisticRegression(max_iter=10000)]\n",
    "        grid['rfe__n_features_to_select'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    \n",
    "    # construct feature type's column transformer\n",
    "    numeric_steps = []\n",
    "    if scaler_flag:      # continous variable normalisation/standardisation\n",
    "        numeric_steps.insert(0, ('scaler', None))\n",
    "        grid['preprocessor__numeric__scaler'] = [MinMaxScaler(), MaxAbsScaler(), StandardScaler(), RobustScaler()]\n",
    "                      \n",
    "    if discretizer_flag:  # continous variable binning\n",
    "        numeric_steps.insert(0, ('discretizer', KBinsDiscretizer(encode='ordinal')))  # ordinal bins\n",
    "        grid['preprocessor__numeric__discretizer__n_bins'] = list(range(2, 11))\n",
    "        grid['preprocessor__numeric__discretizer__strategy'] = ['uniform', 'quantile', 'kmeans']\n",
    "        \n",
    "    numeric_transformer = None\n",
    "    if len(numeric_steps) > 0:\n",
    "        numeric_transformer = Pipeline(steps=numeric_steps)\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[('numeric', numeric_transformer, numeric_features)])\n",
    "        # add numeric ColumnTransformer to global Pipeline\n",
    "        steps.insert(0, ('preprocessor', preprocessor))\n",
    "        \n",
    "    # define the pipeline to evaluate\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the grid search\n",
    "    # search = GridSearchCV(pipeline, grid, scoring='f1_weighted', n_jobs=-1, cv=cv, verbose=2)\n",
    "    search = RandomizedSearchCV(pipeline, grid, scoring=scoring, n_jobs=-1, \n",
    "                                n_iter=n_iter, cv=cv, verbose=2, refit=scoring, random_state=random_state)\n",
    "    \n",
    "    # perform the search\n",
    "    results = search.fit(X_tr, y_tr.values.ravel())\n",
    "    \n",
    "    # summarize best\n",
    "    score = results.best_score_\n",
    "    params = results.best_params_\n",
    "    print('Best Mean F1_weighted: %.3f ' % score)\n",
    "    print('Best Config: %s ' % params)\n",
    "    \n",
    "    # perform classification (linear model doesn't predict an integer value => no predict_proba)\n",
    "    y_pred = search.predict(X_ts)\n",
    "    y_prob = search.predict_proba(X_ts)\n",
    "    \n",
    "    best_features, best_features_scores = [], []\n",
    "    if feature_filter_key != \"\":\n",
    "        best_features, best_features_scores = get_best_features_grid_cv(X_tr, y_tr, results, feature_filter_key)\n",
    "        X_tr = X_tr[best_features]\n",
    "    \n",
    "    # retrieve the tuned model\n",
    "    tuned_model = get_tuned_model(X_tr, y_tr, params)\n",
    "    if tuned_model !=  pipeline['model']:\n",
    "        print(\"Difference in tuned model and pipeline\")\n",
    "        print(\"tuned_model\", tuned_model)\n",
    "        print(\"pipe\", pipeline['model'])\n",
    "        # print(\"estimator\", results.estimator)\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    # plots\n",
    "    if feature_flag and (feature_filter_key == \"\"):\n",
    "        best_features, best_features_scores = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    "    elif (feature_flag) and (feature_filter_key != \"\"):\n",
    "        _ , _ = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    " \n",
    "    cm = confusion_matrix(y_ts, y_pred)\n",
    "    plot_confusion_matrix(cm, results.classes_, True)\n",
    "    plot_classification_report(y_ts, y_pred)\n",
    "    \n",
    "    plot_roc_curve(y_ts, y_prob)\n",
    "    plot_precision_recall_curve(y_ts, y_prob)\n",
    "    plot_cumulative_gain_curve(y_ts, y_prob)\n",
    "    plot_lift_curve_curve(y_ts, y_prob)\n",
    "    \n",
    "    # plot_decision_boundary(X_tr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    # plot_learning_curve(X_tr, y_tr, tuned_model)\n",
    "    # error_visualizer(not_scale_X_ts, y_ts, y_pred, 'chroma_cens_02', 'track_duration')\n",
    "    \n",
    "    return params, tuned_model, y_pred, y_prob, best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alexandra/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "params_tst, tuned_model_tst, y_pred_tst, y_prob_tst, best_features, best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                numeric_features=numeric_columns, \n",
    "                                                                categorical_features=categoric_columns, \n",
    "                                                                discretizer_flag=False,\n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"\", \n",
    "                                                                feature_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\"> <h1> Dimensionality reduction</h1></font>\n",
    "\n",
    "Dimensionality reduction refers to techniques that reduce the number of input variables in a\n",
    "dataset. More input features often make a predictive modeling task more challenging to model,\n",
    "more generally referred to as the **curse of dimensionality**.\n",
    "\n",
    "Dimensionality reduction methods include:\n",
    "- feature selection: use scoring or statistical methods to select which features to keep and which features to delete:\n",
    "    - **Wrapper methods**, wrap a machine learning model, fitting and evaluating the model with different subsets \n",
    "      input features and selecting the subset the results in the best model performance. (i.e. RFE);\n",
    "    - **Filter methods** use scoring methods between the feature and the target variable, to select a\n",
    "      subset of input features that are most predictive (i.e. Pearson’s correlation or Chi-Squared test).\n",
    "- linear algebra methods:\n",
    "    - **Matrix Factorization**: feature matrix (i.e. covariance matrix) can  be ranked and a subset of those parts can       be selected that best captures the salient structure of thematrix that can be used to represent the dataset.       (i.e. PCA);\n",
    "- projection methods / manifold learning and are used to create a low-dimensional projection of high-dimensional data, whilst best preserving the salient structure or relationships in the data (i.e. SOM);\n",
    "- autoencoders: A nueral network model is used that seeks to compress the data flow to a bottleneck layer with far fewer dimensions than the original input data. The part of the model prior to and including the bottleneck is referred to as the encoder, and the part of the model that reads the bottleneck output and reconstructs the input is called the decoder.\n",
    "\n",
    "<u>Typically, linear algebra and manifold learning methods\n",
    "assume that all input features have the same scale or distribution. This suggests that it is\n",
    "good practice to either normalize or standardize data prior to using these methods.</u>\n",
    "\n",
    "<b><font color=\"green\"> \n",
    "For our feature selection process we decided to first use the Pearson Correlation Method to remove highly correlated features (see Features_Data_Understandind.ipynb);\n",
    "Since data dimensionality was still high, we evaluate three different roads:\n",
    "- Univariate Method, using SelectKBest with the ANOVA F-test, since our continous variables are almost alway normal distributed, while our classification target is categorical;\n",
    "- Feature Importance Method, using RFE with features ranked according to feature_importances_ or coef_ derived from a DecisionTreeClassifier() and a LogisticRegression(), respectively;\n",
    "- Variance Threshold, removing features whose variance doesn’t meet a threshold equalt to 0.01\n",
    "</font></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>ANOVA feature selection (filter method)</h2>\n",
    "\n",
    "<u> Importantly, ANOVA is used when one variable is numeric and one is categorical, such as\n",
    "numerical input variables and a classification target variable in a classification task.</u>\n",
    "\n",
    "ANOVA is an acronym for analysis of variance and is a parametric statistical hypothesis test for\n",
    "determining whether the means from two or more samples of data (often three or more) come\n",
    "from the same distribution or not. An F-statistic, or F-test, is a class of statistical tests that\n",
    "calculate the ratio between variances values, such as the variance from two different samples or\n",
    "the explained and unexplained variance by a statistical test, like ANOVA. The ANOVA method\n",
    "is a type of F-statistic referred to here as an **ANOVA F-test**.\n",
    "\n",
    "\n",
    "The scikit-learn machine library provides an implementation of the ANOVA F-test in the\n",
    "f **classif()** function. This function can be used in a feature selection strategy, such as selecting\n",
    "the top k most relevant features (largest values) via the **SelectKBest** class.\n",
    "\n",
    "\n",
    "We cawilln systematically test a range\n",
    "of different numbers of selected features and discover which results in the best performing\n",
    "model. This is called a grid search, where the k argument to the SelectKBest class can be\n",
    "tuned. It is good practice to evaluate model configurations on classification tasks using repeated\n",
    "stratified k-fold cross-validation. We will use **3-fold cross-validation** via the\n",
    "**StratifiedKFold** class.\n",
    "\n",
    "**Since both class labels are equally important and we assign equl cost to FN and FP, we will use as GridSearch's scoring metric the f1_weighted measure** (we want a trade-off among precision and recall).\n",
    "\n",
    "N.B. \n",
    "- f1_macro => unweighted class label\n",
    "- f1_weighted => weighted class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "anova_params_tst, anova_tuned_model_tst, anova_y_pred_tst, anova_y_prob_tst, \\\n",
    "                anova_best_features, anova_best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                numeric_features=numeric_columns, \n",
    "                                                                categorical_features=categoric_columns, \n",
    "                                                                discretizer_flag=False, \n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"anova\", \n",
    "                                                                feature_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>RFE (Recursive Feature Elimination) (wrapper method)</h2>\n",
    "\n",
    "The Recursive Feature Elimination (RFE) method works by recursively removing attributes and building a model on those attributes that remain. It uses accuracy metric to rank the feature according to their importance. The RFE method takes the model to be used and the number of required features as input. It then gives the ranking of all the variables, 1 being most important. It also gives its support, True being relevant feature and False being irrelevant feature.\n",
    "\n",
    "RFE **requires a nested algorithm that is used to provide the feature importance scores**, such\n",
    "as a decision tree (**feature_importances_ attribute**) or a linear model(**coef_ attribute**).\n",
    "\n",
    "Feature importance refers to techniques that assign a score to input features based on how\n",
    "useful they are at predicting a target variable:\n",
    "- statistical correlation scores;\n",
    "- coefficients calculated as part of linear models or decision trees;\n",
    "- permutation importance scores.\n",
    "\n",
    "<u> RFECV automatic select best k number of features </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rfe_params_tst, rfe_tuned_model_tst, rfe_y_pred_tst, rfe_y_prob_tst, \\\n",
    "                rfe_best_features, rfe_best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                numeric_features=numeric_columns, \n",
    "                                                                categorical_features=categoric_columns, \n",
    "                                                                discretizer_flag=False, \n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"rfe\", \n",
    "                                                                feature_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Variance Threshold</h2>\n",
    "\n",
    "Since we have many features, we remove all features whose variance doesn’t meet some threshold. By default, VarianceThreshold removes all zero-variance features, i.e. features that have the same value in all samples.\n",
    "\n",
    "<u>This feature selection algorithm looks only at the features (X), not the desired outputs (y), and can thus be used for unsupervised learning.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vt_params_tst, vt_tuned_model_tst, vt_y_pred_tst, vt_y_prob_tst, \\\n",
    "                vt_best_features, vt_best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr_vt, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts_vt, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                 numeric_features=numeric_columns_vt, \n",
    "                                                                categorical_features=categoric_columns_vt, \n",
    "                                                                discretizer_flag=False, \n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"\", \n",
    "                                                                feature_flag=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Test curves </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_multiple_roc_curves(global_info, model_names, v_or_t_flag):\n",
    "    # draw_roc_auc\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    labels = []\n",
    "    for model_name in global_info.keys():\n",
    "        fpr = global_info[str(model_name)][\"fpr\"]\n",
    "        tpr = global_info[str(model_name)][\"tpr\"]\n",
    "        roc_auc = global_info[str(model_name)][\"roc\"] \n",
    "        label = '%s model=%0.4f' % (model_name, roc_auc)\n",
    "        \n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        labels.append(label)\n",
    "    \n",
    "        # plt.plot(fpr, tpr, label='%s model=%0.4f' % (model_name, roc_auc))\n",
    "        plt.plot(fpr, tpr, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', color=\"k\", label='No-skilled model=0.5000') \n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate') \n",
    "    plt.tick_params(axis='both', which='major')\n",
    "    plt.legend(loc=\"lower right\", title=\"Weighted AUC\", frameon=True)\n",
    "    plt.title(\"%s' %s ROC-curve\" % (model_names, v_or_t_flag))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc, roc_curve, roc_auc_score \n",
    "\n",
    "def get_roc_curve_and_roc_auc_score(y_tst, y_prd, curr_model_name, global_info):\n",
    "    fpr, tpr, thresholds = roc_curve(y_tst, y_prd)\n",
    "    roc = roc_auc_score(y_tst, y_prd, average=\"weighted\")\n",
    "    \n",
    "    global_info[str(curr_model_name)][\"fpr\"] = fpr\n",
    "    global_info[str(curr_model_name)][\"tpr\"] = tpr\n",
    "    global_info[str(curr_model_name)][\"roc\"] = roc\n",
    "    \n",
    "    return global_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_info = {}\n",
    "global_info[\"Plain\"] = {}\n",
    "global_info[\"ANOVA\"] = {}\n",
    "global_info[\"RFE\"] = {}\n",
    "global_info[\"VarianceThreshold\"] = {}\n",
    "\n",
    "global_info = get_roc_curve_and_roc_auc_score(y_test.values.ravel(), y_prob_tst[:, 1], \"Plain\", global_info)\n",
    "global_info = get_roc_curve_and_roc_auc_score(y_test.values.ravel(), anova_y_prob_tst[:, 1], \"ANOVA\", global_info)\n",
    "global_info = get_roc_curve_and_roc_auc_score(y_test.values.ravel(), rfe_y_prob_tst[:, 1], \"RFE\", global_info)\n",
    "global_info = get_roc_curve_and_roc_auc_score(y_test.values.ravel(), vt_y_prob_tst[:, 1], \"VarianceThreshold\", global_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_multiple_roc_curves(global_info, model_name + \"s numeric\" , v_or_t_flag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning curves </h2>\n",
    "\n",
    "This plotting is done at the end of the notebook, beacuse for some reason yellowbrick library overwrite scikitplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Saving best model on file</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_info = {'model_name': 'Plain',\n",
    "              'params': params_tst,\n",
    "              'tuned_model': tuned_model_tst,\n",
    "              'y_pred': y_pred_tst,\n",
    "              'y_prob': y_prob_tst,\n",
    "              'best_features': best_features,\n",
    "              'best_features_scores': best_features_scores\n",
    "             }\n",
    "\n",
    "with open('pickle/bagging_' + model_name + '_numeric.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
