{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Authors</b>: [Alexandra Bradan](https://github.com/alexandrabradan), [Alice Graziani](https://github.com/alicegraziani25) and [Eleonora Cocciu](https://github.com/eleonoracocciu)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 21/05/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The text.latex.preview rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The mathtext.fallback_to_cm rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: Support for setting the 'mathtext.fallback_to_cm' rcParam is deprecated since 3.3 and will be removed two minor releases later; use 'mathtext.fallback : 'cm' instead.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The validate_bool_maybe_none function was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The savefig.jpeg_quality rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The keymap.all_axes rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_path rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n",
      "In /home/alexandra/.local/lib/python3.6/site-packages/matplotlib/mpl-data/stylelib/_classic_test.mplstyle: \n",
      "The animation.avconv_args rcparam was deprecated in Matplotlib 3.3 and will be removed two minor releases later.\n"
     ]
    }
   ],
   "source": [
    "# system library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "# useful libraries\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import statistics\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import percentile\n",
    "\n",
    "# visualisarion\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import Image\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# dimensional reducers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif  # classification\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression  # regression\n",
    "\n",
    "# scalers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# performance visualisation \n",
    "from sklearn import tree\n",
    "from scikitplot.metrics import plot_roc\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# tree classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# linear classifiers\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# neighbors classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# naive_bayes classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# ensemble classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# based_ruled\n",
    "import Orange\n",
    "from Orange.data import *\n",
    "import wittgenstein as lw\n",
    "from skater.core.global_interpretation.interpretable_models.brlc import BRLC\n",
    "from skater.core.global_interpretation.interpretable_models.bigdatabrlc import BigDataBRLC\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "%matplotlib inline\n",
    "\n",
    "from yellowbrick.style import set_palette\n",
    "set_palette('bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Global parameters </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "scoring = 'f1_weighted'\n",
    "random_state = 42\n",
    "\n",
    "# test_n_splits = 9\n",
    "test_n_splits = 3\n",
    "\n",
    "cn2_learner = Orange.classification.rules.CN2Learner() \n",
    "model_name = \"BRLC\"  \n",
    "\n",
    "learning_curve_flag = False\n",
    "v_or_t_flag = \"TST\"\n",
    "cmap = plt.cm.spring_r\n",
    "color = \"fuchsia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Datasets loading </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 55) (10874, 55)\n",
      "(92834, 1) (10874, 1)\n",
      "(92834, 28) (10874, 28)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('../../data/fma_metadata/X_train_merged.csv', index_col=0)\n",
    "X_test = pd.read_csv('../../data/fma_metadata/X_test.csv', index_col=0)\n",
    "\n",
    "y_train = pd.read_csv('../../data/fma_metadata/y_train_merged.csv', index_col=0)\n",
    "y_test = pd.read_csv('../../data/fma_metadata/y_test.csv', index_col=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train_vt = pd.read_csv('../../data/fma_metadata/X_train_merged_vt.csv', index_col=0)\n",
    "X_test_vt = pd.read_csv('../../data/fma_metadata/X_test_vt.csv', index_col=0)\n",
    "\n",
    "print(X_train_vt.shape, X_test_vt.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Continous, categorical/ordinal column retrieval</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns 37\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = []  # continous variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" not in column_name) and  \\\n",
    "          (\"track_date_created_year\" not in column_name) and \\\n",
    "            (\"track_date_created_season\" not in column_name):\n",
    "                numeric_columns.append(column_name)\n",
    "print(\"numeric_columns\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns 18\n"
     ]
    }
   ],
   "source": [
    "categoric_columns = []  # ordinal or categorical variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" in column_name) or  \\\n",
    "          (\"track_date_created_year\" in column_name) or \\\n",
    "            (\"track_date_created_season\" in column_name):\n",
    "                categoric_columns.append(column_name)\n",
    "print(\"categoric_columns\", len(categoric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns_vt 15\n"
     ]
    }
   ],
   "source": [
    "numeric_columns_vt = list(set(numeric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"numeric_columns_vt\", len(numeric_columns_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns_vt 13\n"
     ]
    }
   ],
   "source": [
    "categoric_columns_vt = list(set(categoric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"categoric_columns_vt\", len(categoric_columns_vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Discretize training and test</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 55) (10874, 55) (92834, 28) (10874, 28)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_train.copy()\n",
    "y_tr = y_train.copy()\n",
    "X_ts = X_test.copy()\n",
    "y_ts = y_test.copy()\n",
    "\n",
    "X_tr_vt = X_train_vt.copy()\n",
    "X_ts_vt = X_test_vt.copy()\n",
    "\n",
    "print(X_tr.shape, X_ts.shape, X_tr_vt.shape, X_ts_vt.shape)\n",
    "print(y_tr.shape, y_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genres_columns = [x for x in X_train.columns if \"genre_top\" in x]\\n\\ni = 1\\ngenres_map = {}\\nfor column_name in genres_columns:\\n    genres_map[column_name] = i\\n    i += 1\\n    \\nordinal_genre_top_column_tr = []\\nfor row_idx in X_train.index:\\n    for column_name in genres_columns:\\n        if X_train.at[row_idx, column_name] == 1:\\n            ordinal_genre_top_column_tr.append(genres_map[column_name])\\n            \\nordinal_genre_top_column_ts = []\\nfor row_idx in X_test.index:\\n    for column_name in genres_columns:\\n        if X_test.at[row_idx, column_name] == 1:\\n            ordinal_genre_top_column_ts.append(genres_map[column_name])\\n            \\nif ((len(ordinal_genre_top_column_tr) == X_train.shape[0]) == False) or         ((len(ordinal_genre_top_column_ts) == X_test.shape[0]) == False):\\n    print(len(ordinal_genre_top_column_tr), X_train.shape[0])\\n    print(len(ordinal_genre_top_column_ts), X_test.shape[0])\\n    sys.exit(-1)\\n    \\n# removing onehotencoded columns and inserting new, ordinal one\\ngenres_columns_indeces = []\\nfor column_name in genres_columns:\\n    idx = X_train.columns.get_loc(column_name)\\n    genres_columns_indeces.append(idx)\\n    ()\\nfor column_name in genres_columns:\\n    del X_tr[column_name]\\n    del X_ts[column_name]\\nX_tr.insert(genres_columns_indeces[0], \\'track_genre_top\\', ordinal_genre_top_column_tr)\\nX_ts.insert(genres_columns_indeces[0], \\'track_genre_top\\', ordinal_genre_top_column_ts)\\n\\nprint(X_tr.shape, X_ts.shape)\\nprint(y_tr.shape, y_ts.shape)'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reunite together track_genre_top and make it ordinal\n",
    "\"\"\"genres_columns = [x for x in X_train.columns if \"genre_top\" in x]\n",
    "\n",
    "i = 1\n",
    "genres_map = {}\n",
    "for column_name in genres_columns:\n",
    "    genres_map[column_name] = i\n",
    "    i += 1\n",
    "    \n",
    "ordinal_genre_top_column_tr = []\n",
    "for row_idx in X_train.index:\n",
    "    for column_name in genres_columns:\n",
    "        if X_train.at[row_idx, column_name] == 1:\n",
    "            ordinal_genre_top_column_tr.append(genres_map[column_name])\n",
    "            \n",
    "ordinal_genre_top_column_ts = []\n",
    "for row_idx in X_test.index:\n",
    "    for column_name in genres_columns:\n",
    "        if X_test.at[row_idx, column_name] == 1:\n",
    "            ordinal_genre_top_column_ts.append(genres_map[column_name])\n",
    "            \n",
    "if ((len(ordinal_genre_top_column_tr) == X_train.shape[0]) == False) or \\\n",
    "        ((len(ordinal_genre_top_column_ts) == X_test.shape[0]) == False):\n",
    "    print(len(ordinal_genre_top_column_tr), X_train.shape[0])\n",
    "    print(len(ordinal_genre_top_column_ts), X_test.shape[0])\n",
    "    sys.exit(-1)\n",
    "    \n",
    "# removing onehotencoded columns and inserting new, ordinal one\n",
    "genres_columns_indeces = []\n",
    "for column_name in genres_columns:\n",
    "    idx = X_train.columns.get_loc(column_name)\n",
    "    genres_columns_indeces.append(idx)\n",
    "    ()\n",
    "for column_name in genres_columns:\n",
    "    del X_tr[column_name]\n",
    "    del X_ts[column_name]\n",
    "X_tr.insert(genres_columns_indeces[0], 'track_genre_top', ordinal_genre_top_column_tr)\n",
    "X_ts.insert(genres_columns_indeces[0], 'track_genre_top', ordinal_genre_top_column_ts)\n",
    "\n",
    "print(X_tr.shape, X_ts.shape)\n",
    "print(y_tr.shape, y_ts.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"for column_name in categoric_columns:\\n    X_tr[column_name] = X_tr[column_name].astype('category')\\n    X_ts[column_name] = X_ts[column_name].astype('category')\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set category type to categorical and ordinal values\n",
    "# categoric_columns = [\"track_genre_top\", \"track_date_created_season\", \"track_date_created_year\"]\n",
    "\"\"\"for column_name in categoric_columns:\n",
    "    X_tr[column_name] = X_tr[column_name].astype('category')\n",
    "    X_ts[column_name] = X_ts[column_name].astype('category')\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> Bayesina Rule List Classifier </h2>\n",
    "\n",
    "BRLC(Bayesian Rule List Classifier) is a python wrapper for SBRL(Scalable Bayesian Rule list).\n",
    "SBRL is a scalable Bayesian Rule List. It's a generative estimator to build hierarchical interpretable\n",
    "decision lists.\n",
    "\n",
    "**Default parameters**:\n",
    "  Parameters\n",
    "    ----------\n",
    "    iterations : int (default=30000)\n",
    "        number of iterations for each MCMC chain.\n",
    "    pos_sign : int (default=1)\n",
    "        sign for the positive labels in the \"label\" column.\n",
    "    neg_sign : int (default=0)\n",
    "        sign for the negative labels in the \"label\" column.\n",
    "    min_rule_len : int (default=1)\n",
    "        minimum number of cardinality for rules to be mined from the data-frame.\n",
    "    max_rule_len : int (default=8)\n",
    "        maximum number of cardinality for rules to be mined from the data-frame.\n",
    "    min_support_pos : float (default=0.1)\n",
    "        a number between 0 and 1, for the minimum percentage support for the positive observations.\n",
    "    min_support_neg : float (default 0.1)\n",
    "        a number between 0 and 1, for the minimum percentage support for the negative observations.\n",
    "    eta : int (default=1)\n",
    "        a hyper-parameter for the expected cardinality of the rules in the optimal rule list.\n",
    "    n_chains : int (default=10)\n",
    "        number of chains\n",
    "    alpha : int (default=1)\n",
    "        a prior pseudo-count for the positive(alpha1) and negative(alpha0) classes. \\\n",
    "        Default values (1, 1)\n",
    "    lambda_ : int (default=8)\n",
    "        a hyper-parameter for the expected length of the rule list.\n",
    "    discretize : bool (default=True)\n",
    "        apply discretizer to handle continuous features.\n",
    "    drop_features : bool (default=False)\n",
    "        once continuous features are discretized, use this flag to either retain or drop them from the dataframe\n",
    "\n",
    "**Tuned parameters**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_grid(X_tr):\n",
    "    grid = dict()\n",
    "    grid['model__n_discretize_bins'] = list(range(1, 11))\n",
    "    grid['model__prune_size'] = list(np.arange(0.1, 1.1, 0.1))\n",
    "    grid['model__k'] = [1, 2]\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FUNCTIONS\n",
    "def plot_roc_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_roc(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s ROC curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_precision_recall_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_precision_recall(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Precision-Recall curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cumulative_gain_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_cumulative_gain(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Cumulative Gains curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lift_curve_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_lift_curve(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Lift curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"%s\\'s %s classification report\" % (model_name, v_or_t_flag.upper()))\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_classification_report(y_ts, y_pred):\n",
    "    model_report = classification_report(y_ts, \n",
    "                                       y_pred,\n",
    "                                       # target_names=['Studio Recording', 'Live Recording'],\n",
    "                                       output_dict=True)\n",
    "    model_r = pd.DataFrame(model_report).iloc[:-1, :].T\n",
    "    sns.heatmap(model_r, annot=True, cmap=cmap, cbar=False)\n",
    "    plt.title(\"%s\\'s %s confusion matrix\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_decision_boundary(X_tr, y_tr, scaler, model):\n",
    "    try:\n",
    "        pca = PCA(n_components=2)\n",
    "        best_visualisation_scaler = StandardScaler()\n",
    "        scaled_X_tr = best_visualisation_scaler.fit_transform(X_tr)\n",
    "        X = pca.fit_transform(scaled_X_tr)\n",
    "        # X = pca.fit_transform(X_tr)\n",
    "        y = y_tr.values.ravel()\n",
    "\n",
    "        model.fit(X, y)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        fig = plot_decision_regions(X=X, y=y, clf=model, legend=2)\n",
    "        plt.xlabel(\"PCA component 1\")\n",
    "        plt.ylabel(\"PCA component 2\")\n",
    "        plt.title(\"%s's %s decision boundary\" % (model_name, v_or_t_flag))\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "def spot_errors(test_label, test_pred):  \n",
    "    spot_errors = []\n",
    "    label_errors = []\n",
    "    for i in range(len(test_label)):\n",
    "        if test_label[i] != test_pred[i]:\n",
    "            spot_errors.append('darkred')\n",
    "            label_errors.append(\"wrong prediction\")\n",
    "        else:\n",
    "            spot_errors.append('darkgray')\n",
    "            label_errors.append(\"correct prediction\")\n",
    "    return spot_errors, label_errors\n",
    "\n",
    "def classification_visualizer(test_set, test_label, test_pred):\n",
    "    test_label = test_label.values\n",
    "    \n",
    "    f, axs = plt.subplots(nrows=1, ncols=3, figsize=(24,8))\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    labels = [test_label, test_pred, errors]\n",
    "    titles = ['True Labels', 'Predicted Labels', 'Misclassifications']\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        axs[i].scatter(test_set[missclassif_column_name1], test_set[missclassif_column_name2], c=labels[i], cmap='cividis')\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlabel(missclassif_column_name1, fontdict={'fontsize': 'large'})\n",
    "        axs[i].set_ylabel(missclassif_column_name2, fontdict={'fontsize': 'large'})\n",
    "        \n",
    "    plt.suptitle('Visualization of the ' + model_name + ' classifier on the %s' % v_or_t_flag)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def error_visualizer(test_set, test_label, test_pred, column_name1, column_name2):\n",
    "    test_label = test_label.values\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    \n",
    "    palette = ['darkgray', 'darkred']\n",
    "    if errors[0] == 'darkred':\n",
    "        palette = ['darkred', 'darkgray']\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 6))\n",
    "    sns.scatterplot(x=test_set[column_name1], y=test_set[column_name2], hue=label_errors, palette=palette)\n",
    "    plt.title('%s\\'s %s misclassifications' % (model_name, v_or_t_flag))\n",
    "    plt.xlabel(column_name1)\n",
    "    plt.ylabel(column_name2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_learning_curve(X_tr, y_tr, model, v_or_t_flag):\n",
    "    second_score = \"\"\n",
    "    if v_or_t_flag == 'VAL':\n",
    "        cv = StratifiedKFold(n_splits=validation_n_splits)\n",
    "        second_score = \"Validation score\"\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "        second_score = \"Test score\"\n",
    "    sizes = np.linspace(0.3, 1.0, 10)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    visualizer = LearningCurve(model, cv=cv, scoring=scoring, train_sizes=sizes, \n",
    "                                                                        random_state=random_state)\n",
    "\n",
    "    visualizer.fit(X_tr, y_tr.values.ravel())    \n",
    "    # visualizer.ax.get_lines()[1].set_label(second_score)\n",
    "    visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features_grid_cv(X_tr, y_tr, results, key):\n",
    "    \n",
    "    if key == 'anova':\n",
    "        best_k = results.best_params_['anova__k']\n",
    "        select_k_best = SelectKBest(score_func=f_classif, k=best_k)\n",
    "        fit = select_k_best.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.scores_)\n",
    "        df_columns = pd.DataFrame(X_tr.columns)\n",
    "    elif key == 'rfe':\n",
    "        best_k = results.best_params_['rfe__n_features_to_select']\n",
    "        estimator = results.best_params_['rfe__estimator']\n",
    "        select_rfe = RFE(estimator=estimator, n_features_to_select=best_k)  # best_k=estimator\n",
    "        fit = select_rfe.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.ranking_)\n",
    "    else:\n",
    "        print(\"wrong key=%s\" % key)\n",
    "        sys.exit(-1)\n",
    "  \n",
    "    df_columns = pd.DataFrame(X_tr.columns)\n",
    "    feature_scores = pd.concat([df_columns, df_scores],axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance using %s\" % (model_name, v_or_t_flag,  key.upper()))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances_or_coef(X_tr, y_tr, tuned_model):\n",
    "\n",
    "    try:\n",
    "        df_scores = pd.DataFrame(tuned_model.feature_importances_)\n",
    "        best_k = tuned_model.n_features_\n",
    "    except:\n",
    "        try:\n",
    "            df_scores = pd.DataFrame(tuned_model.coef_)\n",
    "\n",
    "            best_k = len([x for x in list(df_scores.values)])\n",
    "        except:\n",
    "            print(\"Wrong curr_model's retrieval feature importance\")\n",
    "            sys.exit(-1)\n",
    "\n",
    "    df_columns = pd.DataFrame(X_tr.columns)   \n",
    "    feature_scores = pd.concat([df_columns, df_scores], axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance\" % (model_name, v_or_t_flag))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_categorical_feature_with_dummy_ones(df, column_name, categories_list, dummy_features):\n",
    "    \"\"\"\n",
    "    Function which replaces the nominal feature passed by argument with dummy ones, \n",
    "    to convert nominal column's M values in M new binary (dummy) features.\n",
    "    \"\"\"\n",
    "    # retrive nominal feature's index. It is used to know where to insert the new M binary features\n",
    "    index = df.columns.get_loc(column_name)\n",
    "    for i in range(0, dummy_features.shape[1]):\n",
    "        index += 1\n",
    "        df.insert(index, column_name + \"_\" + str(categories_list[i]), \n",
    "                                                              dummy_features[:, i].todense().astype(int), True)\n",
    "    # remove categorical feature\n",
    "    del df[column_name]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_model(X_tr, y_tr, params, numeric_features):\n",
    "    \n",
    "    X_tr_curr = X_tr.copy()\n",
    "    \n",
    "    model_params = list(model_grid(X_tr_curr).keys())\n",
    "    try:\n",
    "        n_bins = params['preprocessor__numeric__discretizer__n_bins'] \n",
    "        strategy = params['preprocessor__numeric__discretizer__strategy']\n",
    "        encode = params['preprocessor__numeric__discretizer__encode']\n",
    "        discretizer = KBinsDiscretizer(encode=encode, n_bins=n_bins, strategy=strategy)\n",
    "        # scale data\n",
    "        X_tr_curr = discretizer.fit_transform(X_tr_curr.values)\n",
    "        \n",
    "        # need to onehot continous and ordinal features\n",
    "        ordinal_columns = ['track_date_created_season', 'track_date_created_year'] \n",
    "        numeric_features = numeric_features + ordinal_columns\n",
    "        for column_name in numeric_features:\n",
    "            try:\n",
    "                categories_list = sorted(list(X_tr_curr[column_name].unique()))\n",
    "                encoder = OneHotEncoder(categories=[categories_list])   \n",
    "                dummy_features = encoder.fit_transform(X_tr_curr[column_name].values.reshape(-1,1))  \n",
    "                X_tr_curr = replace_categorical_feature_with_dummy_ones(X_tr_curr, column_name, categories_list, dummy_features)\n",
    "            except:\n",
    "                continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        scaler = params['preprocessor__numeric__discretizer__scaler']\n",
    "        # scale data\n",
    "        X_tr_curr = scaler.fit_transform(X_tr_curr.values)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # retrieve best hyperameters\n",
    "    tmp_model_hyperparameters = dict((k, params[k]) for k in model_params if k in params)\n",
    "    model_hyperparameters = {}\n",
    "    for key, value in tmp_model_hyperparameters.items():\n",
    "        key = key.split('model__')[1].replace(\"'\", \"\")\n",
    "        model_hyperparameters[key] = value\n",
    "        \n",
    "    tuned_model =  model.set_params(**model_hyperparameters)\n",
    "    tuned_model.fit(X_tr_curr, y_tr.values.ravel())\n",
    "    \n",
    "    plot_decision_boundary(X_tr_curr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    \n",
    "    if learning_curve_flag:\n",
    "        plot_learning_curve(curr_X_tr_vt, y_tr, tuned_model, v_or_t_flag)\n",
    "    \n",
    "    return tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_tr, y_tr, X_ts, y_ts, numeric_features, categorical_features, discretizer_flag, \n",
    "                                                            scaler_flag, feature_filter_key, feature_flag):\n",
    "    \n",
    "    # define the evaluation method\n",
    "    cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "\n",
    "    # construct the pipeline to evaluate\n",
    "    # scaler = RobustScaler()\n",
    "    grid=model_grid(X_tr)\n",
    "    steps = [('model', model)]\n",
    "        \n",
    "    if feature_filter_key == 'anova':\n",
    "        anova = SelectKBest(score_func=f_classif)\n",
    "        steps.insert(0, ('anova', anova))\n",
    "        grid['anova__k'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    elif feature_filter_key == 'rfe':\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier())\n",
    "        steps.insert(0, ('rfe', rfe))\n",
    "        grid['rfe__estimator'] = [DecisionTreeClassifier(), LogisticRegression(max_iter=10000)]\n",
    "        grid['rfe__n_features_to_select'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    \n",
    "    # construct feature type's column transformer\n",
    "    numeric_steps = []\n",
    "    if scaler_flag:      # continous variable normalisation/standardisation\n",
    "        numeric_steps.insert(0, ('scaler', None))\n",
    "        grid['preprocessor__numeric__scaler'] = [MinMaxScaler(), MaxAbsScaler(), StandardScaler(), RobustScaler()]\n",
    "        \n",
    "    ordinal_features = None\n",
    "    ordinal_transformer = None\n",
    "    if discretizer_flag:  # continous variable binning\n",
    "        numeric_steps.insert(0, ('discretizer', KBinsDiscretizer(encode='ordinal')))  # ordinal bins\n",
    "        grid['preprocessor__numeric__discretizer__n_bins'] = list(range(2, 11))\n",
    "        grid['preprocessor__numeric__discretizer__strategy'] = ['uniform', 'quantile', 'kmeans']\n",
    "        \n",
    "        # onehot continous, discretized features\n",
    "        numeric_steps.append(OneHotEncoder())\n",
    "        \n",
    "        # onehot ordinal features\n",
    "        ordinal_features = [\"track_date_created_year\", \"track_date_created_season\"]\n",
    "        ordinal_transformer = OneHotEncoder()                 \n",
    "        \n",
    "    numeric_transformer = None\n",
    "    if len(numeric_steps) > 0:\n",
    "        numeric_transformer = Pipeline(steps=numeric_steps)\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[('numeric', numeric_transformer, numeric_features),\n",
    "                      ('ordinal', ordinal_transformer, ordinal_features)])\n",
    "        # add numeric ColumnTransformer to global Pipeline\n",
    "        steps.insert(0, ('preprocessor', preprocessor))\n",
    "        \n",
    "    # define the pipeline to evaluate\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the grid search\n",
    "    # search = GridSearchCV(pipeline, grid, scoring='f1_weighted',  cv=cv, verbose=1)\n",
    "    search = RandomizedSearchCV(pipeline, grid, scoring=scoring,  \n",
    "                                n_iter=n_iter, cv=cv, verbose=1, refit=scoring, random_state=random_state)\n",
    "    \n",
    "    # perform the search\n",
    "    results = search.fit(X_tr, y_tr.values.ravel())\n",
    "    \n",
    "    # summarize best\n",
    "    score = results.best_score_\n",
    "    params = results.best_params_\n",
    "    print('Best Mean F1_weighted: %.3f ' % score)\n",
    "    print('Best Config: %s ' % params)\n",
    "    \n",
    "    # perform classification (linear model doesn't predict an integer value => no predict_proba)\n",
    "    y_pred = search.predict(X_ts)\n",
    "    y_prob = search.predict_proba(X_ts)\n",
    "    \n",
    "    # performance plots\n",
    "    cm = confusion_matrix(y_ts, y_pred)\n",
    "    plot_confusion_matrix(cm, results.classes_, True)\n",
    "    plot_classification_report(y_ts, y_pred)\n",
    "    \n",
    "    plot_roc_curve(y_ts, y_prob)\n",
    "    plot_precision_recall_curve(y_ts, y_prob)\n",
    "    plot_cumulative_gain_curve(y_ts, y_prob)\n",
    "    plot_lift_curve_curve(y_ts, y_prob)\n",
    "    \n",
    "    best_features, best_features_scores = [], []\n",
    "    if feature_filter_key != \"\":\n",
    "        best_features, best_features_scores = get_best_features_grid_cv(X_tr, y_tr, results, feature_filter_key)\n",
    "        X_tr = X_tr[best_features]\n",
    "    \n",
    "    # retrieve the tuned model\n",
    "    # tuned_model = get_tuned_model(X_tr, y_tr, params, numeric_features)\n",
    "    tuned_model = pipeline['model']\n",
    "    if tuned_model !=  pipeline['model']:\n",
    "        print(\"Difference in tuned model and pipeline\")\n",
    "        print(\"tuned_model\", tuned_model)\n",
    "        print(\"pipe\", pipeline['model'])\n",
    "        # print(\"estimator\", results.estimator)\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    # plots\n",
    "    if feature_flag and (feature_filter_key == \"\"):\n",
    "        best_features, best_features_scores = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    "    elif (feature_flag) and (feature_filter_key != \"\"):\n",
    "        _ , _ = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    "    \n",
    "    plot_decision_boundary(X_tr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    plot_learning_curve(X_tr, y_tr, tuned_model)\n",
    "    # error_visualizer(not_scale_X_ts, y_ts, y_pred, 'chroma_cens_02', 'track_duration')\n",
    "    \n",
    "    return params, tuned_model, y_pred, y_prob, best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'params_tst, tuned_model_tst, y_pred_tst, y_prob_tst, best_features, best_features_scores =                                                     grid_search(X_tr=X_tr, \\n                                                                y_tr=y_tr,\\n                                                                X_ts=X_ts, \\n                                                                y_ts=y_ts,\\n                                                                numeric_features=numeric_columns, \\n                                                                categorical_features=categoric_columns, \\n                                                                discretizer_flag=False,\\n                                                                scaler_flag=False, \\n                                                                feature_filter_key=\"\", \\n                                                                feature_flag=False)'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"params_tst, tuned_model_tst, y_pred_tst, y_prob_tst, best_features, best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                numeric_features=numeric_columns, \n",
    "                                                                categorical_features=categoric_columns, \n",
    "                                                                discretizer_flag=False,\n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"\", \n",
    "                                                                feature_flag=False)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sbrl_model = BRLC(iterations=1000, min_rule_len=1, max_rule_len=X_tr.shape[0], n_chains=X_tr.shape[0], drop_features=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model, by default discretizer is enabled. So, you wish to exclude features then exclude them using\n",
    "# the undiscretize_feature_list parameter\n",
    "model = sbrl_model.fit(X_tr, y_tr.values.ravel()) # bin_labels=\"default\",  undiscretize_feature_list=categoric_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning curves </h2>\n",
    "\n",
    "This plotting is done at the end of the notebook, beacuse for some reason yellowbrick library overwrite scikitplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Saving best model on file</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_info = {'model_name': 'VarianceThreshold',\n",
    "              'params': vt_params_tst,\n",
    "              'tuned_model': vt_tuned_model_tst,\n",
    "              'y_pred': vt_y_pred_tst,\n",
    "              'y_prob': vt_y_prob_tst,\n",
    "              'best_features': vt_best_features,\n",
    "              'best_features_scores': vt_best_features_scores\n",
    "             }\n",
    "\n",
    "with open('pickle/' + model_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
