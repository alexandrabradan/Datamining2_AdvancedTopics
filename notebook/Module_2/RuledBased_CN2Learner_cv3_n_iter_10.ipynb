{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Data mining project - 2020/21</b><br>\n",
    "<b>Authors</b>: [Alexandra Bradan](https://github.com/alexandrabradan), [Alice Graziani](https://github.com/alicegraziani25) and [Eleonora Cocciu](https://github.com/eleonoracocciu)<br>\n",
    "<b>Python version</b>: 3.x<br>\n",
    "<b>Last update: 21/05/2021<b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system library\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# useful libraries\n",
    "import math\n",
    "import operator\n",
    "import itertools\n",
    "import statistics\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import OrderedDict\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "from numpy import std\n",
    "from numpy import mean\n",
    "from numpy import percentile\n",
    "\n",
    "# visualisarion\n",
    "import pydotplus\n",
    "import seaborn as sns\n",
    "from matplotlib import colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from IPython.display import Image\n",
    "\n",
    "# sklearn\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# dimensional reducers\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif  # classification\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression  # regression\n",
    "\n",
    "# scalers\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import MaxAbsScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "# performance visualisation \n",
    "from sklearn import tree\n",
    "from scikitplot.metrics import plot_roc\n",
    "from scikitplot.metrics import plot_precision_recall\n",
    "from scikitplot.metrics import plot_cumulative_gain\n",
    "from scikitplot.metrics import plot_lift_curve\n",
    "from sklearn.model_selection import learning_curve\n",
    "from mlxtend.plotting import plot_decision_regions\n",
    "from yellowbrick.model_selection import LearningCurve\n",
    "\n",
    "# tree classifiers\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# linear classifiers\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# neighbors classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# naive_bayes classifiers\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# ensemble classifiers\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# based_ruled\n",
    "import Orange\n",
    "from Orange.data import *\n",
    "# import wittgenstein as lw\n",
    "\n",
    "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
    "%matplotlib inline\n",
    "\n",
    "from yellowbrick.style import set_palette\n",
    "set_palette('bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Global parameters </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10\n",
    "scoring = 'f1_weighted'\n",
    "random_state = 42\n",
    "\n",
    "# test_n_splits = 9\n",
    "test_n_splits = 3\n",
    "\n",
    "cn2_learner = Orange.classification.rules.CN2Learner() \n",
    "model_name = \"CN2\"  \n",
    "\n",
    "learning_curve_flag = False\n",
    "v_or_t_flag = \"TST\"\n",
    "cmap = plt.cm.spring_r\n",
    "color = \"fuchsia\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Datasets loading </h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 55) (10874, 55)\n",
      "(92834, 1) (10874, 1)\n",
      "(92834, 28) (10874, 28)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.read_csv('../../data/fma_metadata/X_train_merged.csv', index_col=0)\n",
    "X_test = pd.read_csv('../../data/fma_metadata/X_test.csv', index_col=0)\n",
    "\n",
    "y_train = pd.read_csv('../../data/fma_metadata/y_train_merged.csv', index_col=0)\n",
    "y_test = pd.read_csv('../../data/fma_metadata/y_test.csv', index_col=0)\n",
    "\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)\n",
    "\n",
    "X_train_vt = pd.read_csv('../../data/fma_metadata/X_train_merged_vt.csv', index_col=0)\n",
    "X_test_vt = pd.read_csv('../../data/fma_metadata/X_test_vt.csv', index_col=0)\n",
    "\n",
    "print(X_train_vt.shape, X_test_vt.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Continous, categorical/ordinal column retrieval</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns 37\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = []  # continous variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" not in column_name) and  \\\n",
    "          (\"track_date_created_year\" not in column_name) and \\\n",
    "            (\"track_date_created_season\" not in column_name):\n",
    "                numeric_columns.append(column_name)\n",
    "print(\"numeric_columns\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns 18\n"
     ]
    }
   ],
   "source": [
    "categoric_columns = []  # ordinal or categorical variables\n",
    "for column_name in X_train.columns:\n",
    "    if (\"track_genre_top\" in column_name) or  \\\n",
    "          (\"track_date_created_year\" in column_name) or \\\n",
    "            (\"track_date_created_season\" in column_name):\n",
    "                categoric_columns.append(column_name)\n",
    "print(\"categoric_columns\", len(categoric_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numeric_columns_vt 15\n"
     ]
    }
   ],
   "source": [
    "numeric_columns_vt = list(set(numeric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"numeric_columns_vt\", len(numeric_columns_vt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "categoric_columns_vt 13\n"
     ]
    }
   ],
   "source": [
    "categoric_columns_vt = list(set(categoric_columns).intersection(set(X_train_vt.columns)))\n",
    "print(\"categoric_columns_vt\", len(categoric_columns_vt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6> Discretize training and test</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 55) (10874, 55) (92834, 28) (10874, 28)\n",
      "(92834, 1) (10874, 1)\n"
     ]
    }
   ],
   "source": [
    "X_tr = X_train.copy()\n",
    "y_tr = y_train.copy()\n",
    "X_ts = X_test.copy()\n",
    "y_ts = y_test.copy()\n",
    "\n",
    "X_tr_vt = X_train_vt.copy()\n",
    "X_ts_vt = X_test_vt.copy()\n",
    "\n",
    "print(X_tr.shape, X_ts.shape, X_tr_vt.shape, X_ts_vt.shape)\n",
    "print(y_tr.shape, y_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'genres_columns = [x for x in X_train.columns if \"genre_top\" in x]\\n\\ni = 1\\ngenres_map = {}\\nfor column_name in genres_columns:\\n    genres_map[column_name] = i\\n    i += 1\\n    \\nordinal_genre_top_column_tr = []\\nfor row_idx in X_train.index:\\n    for column_name in genres_columns:\\n        if X_train.at[row_idx, column_name] == 1:\\n            ordinal_genre_top_column_tr.append(genres_map[column_name])\\n            \\nordinal_genre_top_column_ts = []\\nfor row_idx in X_test.index:\\n    for column_name in genres_columns:\\n        if X_test.at[row_idx, column_name] == 1:\\n            ordinal_genre_top_column_ts.append(genres_map[column_name])\\n            \\nif ((len(ordinal_genre_top_column_tr) == X_train.shape[0]) == False) or         ((len(ordinal_genre_top_column_ts) == X_test.shape[0]) == False):\\n    print(len(ordinal_genre_top_column_tr), X_train.shape[0])\\n    print(len(ordinal_genre_top_column_ts), X_test.shape[0])\\n    sys.exit(-1)\\n    \\n# removing onehotencoded columns and inserting new, ordinal one\\ngenres_columns_indeces = []\\nfor column_name in genres_columns:\\n    idx = X_train.columns.get_loc(column_name)\\n    genres_columns_indeces.append(idx)\\n    ()\\nfor column_name in genres_columns:\\n    del X_tr[column_name]\\n    del X_ts[column_name]\\nX_tr.insert(genres_columns_indeces[0], \\'track_genre_top\\', ordinal_genre_top_column_tr)\\nX_ts.insert(genres_columns_indeces[0], \\'track_genre_top\\', ordinal_genre_top_column_ts)\\n\\nprint(X_tr.shape, X_ts.shape)\\nprint(y_tr.shape, y_ts.shape)'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reunite together track_genre_top and make it ordinal\n",
    "\"\"\"genres_columns = [x for x in X_train.columns if \"genre_top\" in x]\n",
    "\n",
    "i = 1\n",
    "genres_map = {}\n",
    "for column_name in genres_columns:\n",
    "    genres_map[column_name] = i\n",
    "    i += 1\n",
    "    \n",
    "ordinal_genre_top_column_tr = []\n",
    "for row_idx in X_train.index:\n",
    "    for column_name in genres_columns:\n",
    "        if X_train.at[row_idx, column_name] == 1:\n",
    "            ordinal_genre_top_column_tr.append(genres_map[column_name])\n",
    "            \n",
    "ordinal_genre_top_column_ts = []\n",
    "for row_idx in X_test.index:\n",
    "    for column_name in genres_columns:\n",
    "        if X_test.at[row_idx, column_name] == 1:\n",
    "            ordinal_genre_top_column_ts.append(genres_map[column_name])\n",
    "            \n",
    "if ((len(ordinal_genre_top_column_tr) == X_train.shape[0]) == False) or \\\n",
    "        ((len(ordinal_genre_top_column_ts) == X_test.shape[0]) == False):\n",
    "    print(len(ordinal_genre_top_column_tr), X_train.shape[0])\n",
    "    print(len(ordinal_genre_top_column_ts), X_test.shape[0])\n",
    "    sys.exit(-1)\n",
    "    \n",
    "# removing onehotencoded columns and inserting new, ordinal one\n",
    "genres_columns_indeces = []\n",
    "for column_name in genres_columns:\n",
    "    idx = X_train.columns.get_loc(column_name)\n",
    "    genres_columns_indeces.append(idx)\n",
    "    ()\n",
    "for column_name in genres_columns:\n",
    "    del X_tr[column_name]\n",
    "    del X_ts[column_name]\n",
    "X_tr.insert(genres_columns_indeces[0], 'track_genre_top', ordinal_genre_top_column_tr)\n",
    "X_ts.insert(genres_columns_indeces[0], 'track_genre_top', ordinal_genre_top_column_ts)\n",
    "\n",
    "print(X_tr.shape, X_ts.shape)\n",
    "print(y_tr.shape, y_ts.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(92834, 56) (10874, 56)\n"
     ]
    }
   ],
   "source": [
    "# Place the DataFrames side by side\n",
    "X_tr = pd.concat([X_tr, y_tr], axis=1)\n",
    "X_ts = pd.concat([X_ts, y_ts], axis=1)\n",
    "print(X_tr.shape, X_ts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_performed_encoding(column_name, train_encoded):\n",
    "    encoding_info = {}\n",
    "    for enc, i in zip(train_encoded, X_tr.index):\n",
    "        try:\n",
    "            tmp_list = encoding_info[str(enc)]\n",
    "            tmp_list.append(X_tr.at[i, column_name])\n",
    "            encoding_info[str(enc)] = tmp_list\n",
    "        except KeyError:\n",
    "            encoding_info[str(enc)] = [X_tr.at[i, column_name]]\n",
    "            \n",
    "    for key, value in encoding_info.items():\n",
    "        min_value = min(value)\n",
    "        max_value = max(value)\n",
    "        print(column_name, key, \"[%s-%s]\" %(min_value, max_value),sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform 10 bins discretisation\n",
    "best_k = 10\n",
    "for column_name in numeric_columns:\n",
    "    discretizer = KBinsDiscretizer(n_bins=best_k, encode='ordinal', strategy='uniform')\n",
    "    train_encoded = discretizer.fit_transform(X_train[[column_name]]).astype(int)\n",
    "    test_encoded = discretizer.transform(X_test[[column_name]]).astype(int)\n",
    "    X_tr[column_name] = train_encoded\n",
    "    X_ts[column_name] = test_encoded\n",
    "    # print_performed_encoding(column_name, train_encoded)\n",
    "    \n",
    "# check\n",
    "for column_name in numeric_columns:\n",
    "    if len(X_tr[column_name].unique()) > best_k or len(X_ts[column_name].unique()) > best_k:\n",
    "        print(column_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Load Orange table</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_list = []\n",
    "class_vars = None\n",
    "for column_name in X_tr.columns:\n",
    "    values = []\n",
    "    for elem in X_tr[column_name].unique():\n",
    "        values.append(column_name + \"=\" + str(elem))\n",
    "    var_instance = Orange.data.DiscreteVariable(name=column_name, values=tuple(values))\n",
    "    if column_name == \"album_type\":\n",
    "        class_vars = var_instance\n",
    "    else:\n",
    "         domain_list.append(var_instance)\n",
    "domain = Orange.data.Domain(domain_list, class_vars=class_vars)\n",
    "X_tr_table = Orange.data.Table(domain, X_tr.values)\n",
    "X_ts_table = Orange.data.Table(domain, X_ts.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2> CN2Learner </h2>\n",
    "\n",
    "Induction of rules works by finding a rule that covers some learning instances, removing these instances, and repeating this until all instances are covered. Rules are scored by heuristics such as impurity of class distribution of covered instances. The module includes common rule-learning algorithms, and allows for replacing rule search strategies, scoring and other components.\n",
    "\n",
    "**Default parameters**:\n",
    "\n",
    "\n",
    "**Tuned parameters**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT FUNCTIONS\n",
    "def plot_roc_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_roc(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s ROC curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_precision_recall_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_precision_recall(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Precision-Recall curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_cumulative_gain_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_cumulative_gain(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Cumulative Gains curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_lift_curve_curve(y_ts, y_prob):\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plot_lift_curve(y_ts, y_prob)\n",
    "    plt.title(\"%s\\'s %s Lift curve\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_confusion_matrix(cm, classes, normalize):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(\"%s\\'s %s classification report\" % (model_name, v_or_t_flag.upper()))\n",
    "    # plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    plt.grid(False)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_classification_report(y_ts, y_pred):\n",
    "    model_report = classification_report(y_ts, \n",
    "                                       y_pred,\n",
    "                                       # target_names=['Studio Recording', 'Live Recording'],\n",
    "                                       output_dict=True)\n",
    "    model_r = pd.DataFrame(model_report).iloc[:-1, :].T\n",
    "    sns.heatmap(model_r, annot=True, cmap=cmap, cbar=False)\n",
    "    plt.title(\"%s\\'s %s confusion matrix\" % (model_name, v_or_t_flag.upper()))\n",
    "    plt.show()\n",
    "    \n",
    "def plot_decision_boundary(X_tr, y_tr, scaler, model):\n",
    "    try:\n",
    "        pca = PCA(n_components=2)\n",
    "        best_visualisation_scaler = StandardScaler()\n",
    "        scaled_X_tr = best_visualisation_scaler.fit_transform(X_tr)\n",
    "        X = pca.fit_transform(scaled_X_tr)\n",
    "        # X = pca.fit_transform(X_tr)\n",
    "        y = y_tr.values.ravel()\n",
    "\n",
    "        model.fit(X, y)\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        fig = plot_decision_regions(X=X, y=y, clf=model, legend=2)\n",
    "        plt.xlabel(\"PCA component 1\")\n",
    "        plt.ylabel(\"PCA component 2\")\n",
    "        plt.title(\"%s's %s decision boundary\" % (model_name, v_or_t_flag))\n",
    "        plt.legend(loc='best')\n",
    "        plt.grid(False)\n",
    "        plt.show()\n",
    "    except ValueError:\n",
    "        return\n",
    "\n",
    "def spot_errors(test_label, test_pred):  \n",
    "    spot_errors = []\n",
    "    label_errors = []\n",
    "    for i in range(len(test_label)):\n",
    "        if test_label[i] != test_pred[i]:\n",
    "            spot_errors.append('darkred')\n",
    "            label_errors.append(\"wrong prediction\")\n",
    "        else:\n",
    "            spot_errors.append('darkgray')\n",
    "            label_errors.append(\"correct prediction\")\n",
    "    return spot_errors, label_errors\n",
    "\n",
    "def classification_visualizer(test_set, test_label, test_pred):\n",
    "    test_label = test_label.values\n",
    "    \n",
    "    f, axs = plt.subplots(nrows=1, ncols=3, figsize=(24,8))\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    labels = [test_label, test_pred, errors]\n",
    "    titles = ['True Labels', 'Predicted Labels', 'Misclassifications']\n",
    "    \n",
    "    for i in range(0, 3):\n",
    "        axs[i].scatter(test_set[missclassif_column_name1], test_set[missclassif_column_name2], c=labels[i], cmap='cividis')\n",
    "        axs[i].set_title(titles[i])\n",
    "        axs[i].set_xlabel(missclassif_column_name1, fontdict={'fontsize': 'large'})\n",
    "        axs[i].set_ylabel(missclassif_column_name2, fontdict={'fontsize': 'large'})\n",
    "        \n",
    "    plt.suptitle('Visualization of the ' + model_name + ' classifier on the %s' % v_or_t_flag)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def error_visualizer(test_set, test_label, test_pred, column_name1, column_name2):\n",
    "    test_label = test_label.values\n",
    "    errors, label_errors = spot_errors(test_label, test_pred)\n",
    "    \n",
    "    palette = ['darkgray', 'darkred']\n",
    "    if errors[0] == 'darkred':\n",
    "        palette = ['darkred', 'darkgray']\n",
    "    \n",
    "    fig = plt.figure(figsize=(5, 6))\n",
    "    sns.scatterplot(x=test_set[column_name1], y=test_set[column_name2], hue=label_errors, palette=palette)\n",
    "    plt.title('%s\\'s %s misclassifications' % (model_name, v_or_t_flag))\n",
    "    plt.xlabel(column_name1)\n",
    "    plt.ylabel(column_name2)\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_learning_curve(X_tr, y_tr, model, v_or_t_flag):\n",
    "    second_score = \"\"\n",
    "    if v_or_t_flag == 'VAL':\n",
    "        cv = StratifiedKFold(n_splits=validation_n_splits)\n",
    "        second_score = \"Validation score\"\n",
    "    else:\n",
    "        cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "        second_score = \"Test score\"\n",
    "    sizes = np.linspace(0.3, 1.0, 10)\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    visualizer = LearningCurve(model, cv=cv, scoring=scoring, train_sizes=sizes, \n",
    "                                                                        random_state=random_state)\n",
    "\n",
    "    visualizer.fit(X_tr, y_tr.values.ravel())    \n",
    "    # visualizer.ax.get_lines()[1].set_label(second_score)\n",
    "    visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_features_grid_cv(X_tr, y_tr, results, key):\n",
    "    \n",
    "    if key == 'anova':\n",
    "        best_k = results.best_params_['anova__k']\n",
    "        select_k_best = SelectKBest(score_func=f_classif, k=best_k)\n",
    "        fit = select_k_best.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.scores_)\n",
    "        df_columns = pd.DataFrame(X_tr.columns)\n",
    "    elif key == 'rfe':\n",
    "        best_k = results.best_params_['rfe__n_features_to_select']\n",
    "        estimator = results.best_params_['rfe__estimator']\n",
    "        select_rfe = RFE(estimator=estimator, n_features_to_select=best_k)  # best_k=estimator\n",
    "        fit = select_rfe.fit(X_tr, y_tr.values.ravel())\n",
    "        df_scores = pd.DataFrame(fit.ranking_)\n",
    "    else:\n",
    "        print(\"wrong key=%s\" % key)\n",
    "        sys.exit(-1)\n",
    "  \n",
    "    df_columns = pd.DataFrame(X_tr.columns)\n",
    "    feature_scores = pd.concat([df_columns, df_scores],axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance using %s\" % (model_name, v_or_t_flag,  key.upper()))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importances_or_coef(X_tr, y_tr, tuned_model):\n",
    "\n",
    "    try:\n",
    "        df_scores = pd.DataFrame(tuned_model.feature_importances_)\n",
    "        best_k = tuned_model.n_features_\n",
    "    except:\n",
    "        try:\n",
    "            df_scores = pd.DataFrame(tuned_model.coef_)\n",
    "\n",
    "            best_k = len([x for x in list(df_scores.values)])\n",
    "        except:\n",
    "            print(\"Wrong curr_model's retrieval feature importance\")\n",
    "            sys.exit(-1)\n",
    "\n",
    "    df_columns = pd.DataFrame(X_tr.columns)   \n",
    "    feature_scores = pd.concat([df_columns, df_scores], axis=1) # concatenate dataframes\n",
    "    feature_scores.columns = ['features','scores']  # name output columns\n",
    "    feature_scores = feature_scores[feature_scores['scores'] != 0]  # keeping only non-zero scoring features\n",
    "\n",
    "    # plot feature importance\n",
    "    fig = plt.figure(figsize=(8, 10))\n",
    "    # keeping ongly best_k features, ordered in descending score\n",
    "    ordered_k_feature_scores = feature_scores.sort_values('scores', ascending=False).iloc[:best_k]\n",
    "    sns.barplot(y='features', x='scores', data=feature_scores, color=color,\n",
    "                                                            order=ordered_k_feature_scores.features)\n",
    "    plt.grid(False)\n",
    "    plt.title(\"%s's %s feature importance\" % (model_name, v_or_t_flag))\n",
    "    plt.show()\n",
    "\n",
    "    # retrieve best features \n",
    "    # best_features = [column[0] for column in zip(X_tr.columns, select_k_best.get_support()) if column[1]]\n",
    "    best_features = list(ordered_k_feature_scores.features)\n",
    "    best_features_scores = list(ordered_k_feature_scores.scores)\n",
    "\n",
    "\n",
    "    return best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_categorical_feature_with_dummy_ones(df, column_name, categories_list, dummy_features):\n",
    "    \"\"\"\n",
    "    Function which replaces the nominal feature passed by argument with dummy ones, \n",
    "    to convert nominal column's M values in M new binary (dummy) features.\n",
    "    \"\"\"\n",
    "    # retrive nominal feature's index. It is used to know where to insert the new M binary features\n",
    "    index = df.columns.get_loc(column_name)\n",
    "    for i in range(0, dummy_features.shape[1]):\n",
    "        index += 1\n",
    "        df.insert(index, column_name + \"_\" + str(categories_list[i]), \n",
    "                                                              dummy_features[:, i].todense().astype(int), True)\n",
    "    # remove categorical feature\n",
    "    del df[column_name]\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tuned_model(X_tr, y_tr, params, numeric_features):\n",
    "    \n",
    "    X_tr_curr = X_tr.copy()\n",
    "    \n",
    "    model_params = list(model_grid(X_tr_curr).keys())\n",
    "    try:\n",
    "        n_bins = params['preprocessor__numeric__discretizer__n_bins'] \n",
    "        strategy = params['preprocessor__numeric__discretizer__strategy']\n",
    "        encode = params['preprocessor__numeric__discretizer__encode']\n",
    "        discretizer = KBinsDiscretizer(encode=encode, n_bins=n_bins, strategy=strategy)\n",
    "        # scale data\n",
    "        X_tr_curr = discretizer.fit_transform(X_tr_curr.values)\n",
    "        \n",
    "        # need to onehot continous and ordinal features\n",
    "        ordinal_columns = ['track_date_created_season', 'track_date_created_year'] \n",
    "        numeric_features = numeric_features + ordinal_columns\n",
    "        for column_name in numeric_features:\n",
    "            try:\n",
    "                categories_list = sorted(list(X_tr_curr[column_name].unique()))\n",
    "                encoder = OneHotEncoder(categories=[categories_list])   \n",
    "                dummy_features = encoder.fit_transform(X_tr_curr[column_name].values.reshape(-1,1))  \n",
    "                X_tr_curr = replace_categorical_feature_with_dummy_ones(X_tr_curr, column_name, categories_list, dummy_features)\n",
    "            except:\n",
    "                continue\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "\n",
    "    try:\n",
    "        scaler = params['preprocessor__numeric__discretizer__scaler']\n",
    "        # scale data\n",
    "        X_tr_curr = scaler.fit_transform(X_tr_curr.values)\n",
    "    except KeyError:\n",
    "        pass\n",
    "    \n",
    "    # retrieve best hyperameters\n",
    "    tmp_model_hyperparameters = dict((k, params[k]) for k in model_params if k in params)\n",
    "    model_hyperparameters = {}\n",
    "    for key, value in tmp_model_hyperparameters.items():\n",
    "        key = key.split('model__')[1].replace(\"'\", \"\")\n",
    "        model_hyperparameters[key] = value\n",
    "        \n",
    "    tuned_model =  model.set_params(**model_hyperparameters)\n",
    "    tuned_model.fit(X_tr_curr, y_tr.values.ravel())\n",
    "    \n",
    "    plot_decision_boundary(X_tr_curr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    \n",
    "    if learning_curve_flag:\n",
    "        plot_learning_curve(curr_X_tr_vt, y_tr, tuned_model, v_or_t_flag)\n",
    "    \n",
    "    return tuned_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(X_tr, y_tr, X_ts, y_ts, numeric_features, categorical_features, discretizer_flag, \n",
    "                                                            scaler_flag, feature_filter_key, feature_flag):\n",
    "    \n",
    "    # define the evaluation method\n",
    "    cv = StratifiedKFold(n_splits=test_n_splits)\n",
    "\n",
    "    # construct the pipeline to evaluate\n",
    "    # scaler = RobustScaler()\n",
    "    grid=model_grid(X_tr)\n",
    "    steps = [('model', model)]\n",
    "        \n",
    "    if feature_filter_key == 'anova':\n",
    "        anova = SelectKBest(score_func=f_classif)\n",
    "        steps.insert(0, ('anova', anova))\n",
    "        grid['anova__k'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    elif feature_filter_key == 'rfe':\n",
    "        rfe = RFE(estimator=DecisionTreeClassifier())\n",
    "        steps.insert(0, ('rfe', rfe))\n",
    "        grid['rfe__estimator'] = [DecisionTreeClassifier(), LogisticRegression(max_iter=10000)]\n",
    "        grid['rfe__n_features_to_select'] = [i+1 for i in range(X_tr.shape[1])]\n",
    "    \n",
    "    # construct feature type's column transformer\n",
    "    numeric_steps = []\n",
    "    if scaler_flag:      # continous variable normalisation/standardisation\n",
    "        numeric_steps.insert(0, ('scaler', None))\n",
    "        grid['preprocessor__numeric__scaler'] = [MinMaxScaler(), MaxAbsScaler(), StandardScaler(), RobustScaler()]\n",
    "        \n",
    "    ordinal_features = None\n",
    "    ordinal_transformer = None\n",
    "    if discretizer_flag:  # continous variable binning\n",
    "        numeric_steps.insert(0, ('discretizer', KBinsDiscretizer(encode='ordinal')))  # ordinal bins\n",
    "        grid['preprocessor__numeric__discretizer__n_bins'] = list(range(2, 11))\n",
    "        grid['preprocessor__numeric__discretizer__strategy'] = ['uniform', 'quantile', 'kmeans']\n",
    "        \n",
    "        # onehot continous, discretized features\n",
    "        numeric_steps.append(OneHotEncoder())\n",
    "        \n",
    "        # onehot ordinal features\n",
    "        ordinal_features = [\"track_date_created_year\", \"track_date_created_season\"]\n",
    "        ordinal_transformer = OneHotEncoder()                 \n",
    "        \n",
    "    numeric_transformer = None\n",
    "    if len(numeric_steps) > 0:\n",
    "        numeric_transformer = Pipeline(steps=numeric_steps)\n",
    "        preprocessor = ColumnTransformer(\n",
    "        transformers=[('numeric', numeric_transformer, numeric_features),\n",
    "                      ('ordinal', ordinal_transformer, ordinal_features)])\n",
    "        # add numeric ColumnTransformer to global Pipeline\n",
    "        steps.insert(0, ('preprocessor', preprocessor))\n",
    "        \n",
    "    # define the pipeline to evaluate\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    \n",
    "    # define the grid search\n",
    "    # search = GridSearchCV(pipeline, grid, scoring='f1_weighted',  cv=cv, verbose=1)\n",
    "    search = RandomizedSearchCV(pipeline, grid, scoring=scoring,  \n",
    "                                n_iter=n_iter, cv=cv, verbose=1, refit=scoring, random_state=random_state)\n",
    "    \n",
    "    # perform the search\n",
    "    results = search.fit(X_tr, y_tr.values.ravel())\n",
    "    \n",
    "    # summarize best\n",
    "    score = results.best_score_\n",
    "    params = results.best_params_\n",
    "    print('Best Mean F1_weighted: %.3f ' % score)\n",
    "    print('Best Config: %s ' % params)\n",
    "    \n",
    "    # perform classification (linear model doesn't predict an integer value => no predict_proba)\n",
    "    y_pred = search.predict(X_ts)\n",
    "    y_prob = search.predict_proba(X_ts)\n",
    "    \n",
    "    # performance plots\n",
    "    cm = confusion_matrix(y_ts, y_pred)\n",
    "    plot_confusion_matrix(cm, results.classes_, True)\n",
    "    plot_classification_report(y_ts, y_pred)\n",
    "    \n",
    "    plot_roc_curve(y_ts, y_prob)\n",
    "    plot_precision_recall_curve(y_ts, y_prob)\n",
    "    plot_cumulative_gain_curve(y_ts, y_prob)\n",
    "    plot_lift_curve_curve(y_ts, y_prob)\n",
    "    \n",
    "    best_features, best_features_scores = [], []\n",
    "    if feature_filter_key != \"\":\n",
    "        best_features, best_features_scores = get_best_features_grid_cv(X_tr, y_tr, results, feature_filter_key)\n",
    "        X_tr = X_tr[best_features]\n",
    "    \n",
    "    # retrieve the tuned model\n",
    "    # tuned_model = get_tuned_model(X_tr, y_tr, params, numeric_features)\n",
    "    tuned_model = pipeline['model']\n",
    "    if tuned_model !=  pipeline['model']:\n",
    "        print(\"Difference in tuned model and pipeline\")\n",
    "        print(\"tuned_model\", tuned_model)\n",
    "        print(\"pipe\", pipeline['model'])\n",
    "        # print(\"estimator\", results.estimator)\n",
    "        sys.exit(-1)\n",
    "    \n",
    "    # plots\n",
    "    if feature_flag and (feature_filter_key == \"\"):\n",
    "        best_features, best_features_scores = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    "    elif (feature_flag) and (feature_filter_key != \"\"):\n",
    "        _ , _ = get_feature_importances_or_coef(X_tr, y_tr, tuned_model)\n",
    "    \n",
    "    plot_decision_boundary(X_tr, y_tr, MinMaxScaler(), tuned_model)  # passing random scaler\n",
    "    plot_learning_curve(X_tr, y_tr, tuned_model)\n",
    "    # error_visualizer(not_scale_X_ts, y_ts, y_pred, 'chroma_cens_02', 'track_duration')\n",
    "    \n",
    "    return params, tuned_model, y_pred, y_prob, best_features, best_features_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    }
   ],
   "source": [
    "\"\"\"params_tst, tuned_model_tst, y_pred_tst, y_prob_tst, best_features, best_features_scores = \\\n",
    "                                                    grid_search(X_tr=X_tr, \n",
    "                                                                y_tr=y_tr,\n",
    "                                                                X_ts=X_ts, \n",
    "                                                                y_ts=y_ts,\n",
    "                                                                numeric_features=numeric_columns, \n",
    "                                                                categorical_features=categoric_columns, \n",
    "                                                                discretizer_flag=False,\n",
    "                                                                scaler_flag=False, \n",
    "                                                                feature_filter_key=\"\", \n",
    "                                                                feature_flag=False)\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn2_classifier = cn2_learner(X_tr_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_rules 13900\n"
     ]
    }
   ],
   "source": [
    "set_rules = cn2_classifier.rule_list\n",
    "print(\"num_rules\", len(set_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle/' + model_name + '_rules.pickle', 'wb') as handle:\n",
    "    pickle.dump(cn2_classifier.rule_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Load rules and perform classification</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle/' + model_name + '_rules.pickle', 'rb') as handle:\n",
    "    set_rules = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REAL num_rules 13900\n",
      "rules equal train records? False\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "count_records = 0\n",
    "set_rules_class_dist = []\n",
    "for rule in set_rules:\n",
    "    try:\n",
    "        rule_record_list = rule.curr_class_dist.tolist()\n",
    "        # print(rule, rule_record_list)\n",
    "        set_rules_class_dist.append(rule_record_list)\n",
    "        count_records += sum(rule_record_list)\n",
    "        count += 1\n",
    "    except:\n",
    "        continue\n",
    "print(\"REAL num_rules\", count)\n",
    "print(\"rules equal train records?\", count_records == X_tr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_rule_in_antecedents_and_conseguent(rule):\n",
    "    tmp_list = str(rule).replace(\"IF\", \"\").split(\"THEN\")\n",
    "    antecedents_dict = {}\n",
    "    tmp_antecedent = tmp_list[0].split(\"AND\")\n",
    "    for a in tmp_antecedent:\n",
    "        tmp_a = a.split(\"==\")\n",
    "        if len(tmp_a) == 1:\n",
    "            # tmp_a = tmp_a.split(\"!=\")\n",
    "            continue\n",
    "        antecedent = tmp_a[1].replace(\" \", \"\")\n",
    "        antecedents_dict[antecedent] = None\n",
    "        \"\"\"tmp_a = tmp_a[1].split(\"=\")\n",
    "        antecedent = tmp_a[0]\n",
    "        antecedent_value = tmp_a[1]\n",
    "        antecedents_dict[antecedent] = int(antecedent_value)\"\"\"\n",
    "    conseguent_value = int(tmp_list[1].replace(\"album_type=album_type=\", \"\"))\n",
    "    return antecedents_dict, conseguent_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive_set_rules 4219\n"
     ]
    }
   ],
   "source": [
    "# keeping only positive predictive rules\n",
    "positive_set_rules = []\n",
    "for rule in set_rules:\n",
    "    try:\n",
    "        antecedents_dict, conseguent_value = decompose_rule_in_antecedents_and_conseguent(rule)\n",
    "        if conseguent_value == 1:\n",
    "            positive_set_rules.append(rule)\n",
    "    except:\n",
    "        continue\n",
    "print(\"positive_set_rules\", len(positive_set_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b3b6a05758849a3832805bc130265b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4218.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'Rule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-e12ea831397a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m                  \u001b[0mremoved_positive_set_rules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_rule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0msuper_positive_set_rules\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_set_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdifference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoved_positive_set_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"super_positive_set_rules\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper_positive_set_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"removed_positive_set_rules\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mremoved_positive_set_rules\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'Rule'"
     ]
    }
   ],
   "source": [
    "# filter and keep only super-set rules\n",
    "removed_positive_set_rules = []\n",
    "for i in tqdm(range(0, len(positive_set_rules) - 1)):\n",
    "    curr_rule = positive_set_rules[i]\n",
    "    try:\n",
    "        curr_antecedents_dict, curr_conseguent_value = decompose_rule_in_antecedents_and_conseguent(curr_rule)\n",
    "        curr_keys = set(curr_antecedents_dict.keys())\n",
    "    except:\n",
    "        removed_positive_set_rules.append(curr_rule)\n",
    "        continue\n",
    "    for j in range(i + 1, len(positive_set_rules)):\n",
    "        next_rule = positive_set_rules[j]\n",
    "        if next_rule not in removed_positive_set_rules:\n",
    "            try:\n",
    "                next_antecedents_dict, next_conseguent_value = decompose_rule_in_antecedents_and_conseguent(next_rule)\n",
    "                next_keys = set(next_antecedents_dict.keys())\n",
    "                \n",
    "                # check if curr is sub-rule of next\n",
    "                if len(next_keys.intersection(curr_keys)) == len(curr_keys):\n",
    "                    # disregard curr and keep next (for now)\n",
    "                    removed_positive_set_rules.append(curr_rule)\n",
    "                    break\n",
    "                elif len(next_keys.intersection(curr_keys)) == len(next_keys):  # check if next is sub-rule of curr\n",
    "                    # disregard next and keep xurr (for now)\n",
    "                    removed_positive_set_rules.append(next_rule)\n",
    "            except:\n",
    "                 removed_positive_set_rules.append(next_rule)\n",
    "                    \n",
    "super_positive_set_rules = list(set(positive_set_rules).difference(set(removed_positive_set_rules)))\n",
    "print(\"super_positive_set_rules\", len(super_positive_set_rules))\n",
    "print(\"removed_positive_set_rules\", len(removed_positive_set_rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('pickle/' + model_name + '_super_positive_rules.pickle', 'wb') as handle:\n",
    "    pickle.dump(super_positive_set_rules, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAJORITY VOTE\n",
    "majority_vote = {}\n",
    "for tqdm(idx in X_ts.index):\n",
    "    ts_antecedent = []\n",
    "    for column_name in X_ts.columns():\n",
    "        values = X_ts.at[idx, column_name]\n",
    "        ts_antecedent.append(value)\n",
    "    ts_antecedent = set(ts_antecedent)\n",
    "    \n",
    "    # iterate over each super-set rule and perfom majority vote uppon matching rules\n",
    "    majority_vote[idx] = [0, 0]\n",
    "    for curr_rule in super_positive_set_rules:\n",
    "        curr_antecedents_dict, curr_conseguent_value = decompose_rule_in_antecedents_and_conseguent(curr_rule)\n",
    "        curr_keys = set(curr_antecedents_dict.keys())\n",
    "        if len(ts_antecedent.intersection(ts_antecedent)) == len(ts_antecedent):\n",
    "            majority_vote[idx][curr_conseguent_value] += 1\n",
    "            \n",
    "y_pred = []\n",
    "for idx in tqdm(X_ts.index):\n",
    "    list_counter = majority_vote[idx]\n",
    "    if list_counter[0] > list_counter[1]:\n",
    "        y_pred.append(0)  # predicted negative class\n",
    "    elif list_counter[0] < list_counter[1]:\n",
    "        y_pred.append(1)  # predicted positive class\n",
    "    elif (list_counter[0] == list_counter[1]) and (list_counter[0] != 0) else (list_counter[1] != 0):\n",
    "         y_pred.append(1)  # favor positive class in ties, different than no prediction\n",
    "    else:\n",
    "        y_pred.append(0)   # predict negative class, by deafult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred) == len(y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Learning curves </h2>\n",
    "\n",
    "This plotting is done at the end of the notebook, beacuse for some reason yellowbrick library overwrite scikitplot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h6>Saving best model on file</h6>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_info = {'model_name': 'VarianceThreshold',\n",
    "              'params': vt_params_tst,\n",
    "              'tuned_model': vt_tuned_model_tst,\n",
    "              'y_pred': vt_y_pred_tst,\n",
    "              'y_prob': vt_y_prob_tst,\n",
    "              'best_features': vt_best_features,\n",
    "              'best_features_scores': vt_best_features_scores\n",
    "             }\n",
    "\n",
    "with open('pickle/' + model_name + '.pickle', 'wb') as handle:\n",
    "    pickle.dump(model_info, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
